# Летняя практика Т-Банк

# Анализ цепей постоянного тока (DC Circuit Analysis)

## Описание проекта

Данный проект представляет собой систему для обучения языковых моделей решению задач анализа электрических цепей постоянного тока. Среда проверяет понимание законов Ома и Кирхгофа, что является фундаментальным навыком в электротехнике и отличается от чисто логических или математических задач.

### Ключевые особенности среды:
- **Описание задачи:** Дана схема простой электрической цепи, состоящей из источников напряжения и резисторов, соединенных последовательно и параллельно. Задача — рассчитать определенную величину: ток через конкретный резистор, напряжение на нем или общее сопротивление цепи.
- **Почему это хорошо подходит:** Законы Ома и Кирхгофа сводят анализ цепи к решению системы линейных уравнений. Это строго формальная, алгоритмически решаемая задача.
- **Логика генератора данных:**
    1) Генерируется граф, представляющий схему. Можно начать с простых топологий (последовательная, параллельная) и усложнять до смешанных соединений.
    2) Узлам (источникам) и ребрам (резисторам) присваиваются случайные значения (напряжение в вольтах, сопротивление в омах).
    3) Случайным образом выбирается целевой вопрос: "найти ток через R3", "найти напряжение между точками А и Б".
    4) Сложность (difficulty) контролируется количеством элементов, сложностью топологии и типом вопроса.
- **Логика верификатора:**
    1) Верификатор преобразует граф цепи в систему линейных уравнений на основе правил Кирхгофа.
    2) Решает эту систему, чтобы найти все токи и напряжения в цепи.
    3) Сравнивает вычисленное значение с ответом LLM.

---

## Этап 1: Создание RL среды

### Проблемы начальной реализации

Изначально я реализовал слишком сложную RL среду с избыточными функциями, которые не требовались по ТЗ. 

Это включало в себя:
- Поддержку смешанных цепей с произвольной топологией
- Реализацию полных законов Кирхгофа (KCL, KVL)
- Сложные матричные методы решения систем уравнений
- Избыточные типы вопросов (мощность, эквивалентное сопротивление)

После получения **ужасных** результатов при обучении модели (ожидание обучения более 12 часов, почти безрезультатно), я решил упростить архитектуру и сосредоточиться на базовых топологиях: последовательных и параллельных цепях (из тз).

### Реализованная архитектура

#### Класс DCCircuitGame
Основной класс, покрывающий всю функциональность RL среды:
- Генерация задач через `generate()` метод
- Верификация ответов через `verify()` метод  
- Извлечение ответов через `extract_answer()` метод

#### Генератор цепей (CircuitGenerator)
Создает задачи для RL среды:

**Последовательные цепи:**
```python
def _generate_series(self, num_resistors: int):
    # Создание узлов: A (источник) -> N1 -> N2 -> ... -> B (земля)
    nodes = ["A"] + [f"N{i+1}" for i in range(num_resistors-1)] + ["B"]
    
    # Добавление резисторов в цепочку
    for i in range(num_resistors):
        if i == 0:
            circuit.add_resistor("A", "N1", resistance)
        elif i == num_resistors - 1:
            circuit.add_resistor(f"N{i}", "B", resistance)
        else:
            circuit.add_resistor(f"N{i}", f"N{i+1}", resistance)
```
**Параллельные цепи:**
```python
def _generate_parallel(self, num_resistors: int):
    # Все резисторы подключены между узлами A и B
    for i in range(num_resistors):
        circuit.add_resistor("A", "B", resistance)
```

#### Решатель цепей (CircuitSolver)
Реализует физически корректное решение цепей:

**Для последовательных цепей:**
- Общее сопротивление: R_total = R₁ + R₂ + ... + Rₙ
- Ток: I = V_source / R_total
- Напряжения на узлах: V_node = V_ground + I × ΣR_before

**Для параллельных цепей:**
- Напряжение одинаково на всех резисторах: V = V_source
- Токи через резисторы: I_i = V / R_i

#### Верификатор для reward функции
Реализует градиентную систему оценки:

```python
def get_accuracy_score(self, data: Data, response: str) -> float:
    relative_error = abs(response - data.answer) / abs(data.answer)
    
    if   relative_error <= 0.01:  return 1.0   # ≤1%
    elif relative_error <= 0.05:  return 0.75  # ≤5%
    elif relative_error <= 0.10:  return 0.5   # 5%
    elif relative_error <= 0.20:  return 0.25  # ≤20%
    else:                         return 0.0   # >20%
```

### Системный промпт

Реализован детальный системный промпт для обучения модели:

```
Ты эксперт по анализу электрических цепей постоянного тока.

Используй следующие физические законы:
1. Закон Ома: V = I × R, I = V/R, R = V/I
2. Для последовательных цепей: R_total = R₁ + R₂ + ... + Rₙ
3. Для параллельных цепей: 1/R_total = 1/R₁ + 1/R₂ + ... + 1/Rₙ

Формат ответа:
<think>Твои рассуждения и вычисления</think>
<answer>X.XXX</answer>

Где X.XXX - финальный числовой ответ с точностью до 3 знаков после запятой.
```

### Пример сгенерированной задачи

**Входные данные:**
```
Схема: Последовательная цепь
Источник напряжения: 12V между узлами A и B
Резисторы: R1=4Ω (A-N1), R2=6Ω (N1-B)
Вопрос: Найти напряжение на резисторе R1
```

**Правильный ответ:** 4.800V

### Результаты первого этапа

 **Реализовал класс DCCircuitGame** - покрывает генерацию, верификацию и извлечение ответов  
 **Реализовал CircuitGenerator** - создает последовательные и параллельные цепи  
 **Реализовал CircuitSolver** - физически корректное решение цепей  
 **Реализовал DCCircuitVerifier** - градиентная система оценки  
 **Создал системный промпт** - детальные инструкции для модели  
 **Настроил 6 уровней сложности** - от 2 до 10 резисторов  

---

## Этап 2: Дообучение LLM агента на RL среде

### Выбор модели

Потратил значительное время на выбор подходящей модели. Некоторые модели (например, Qwen3-4B-thinking) уже справлялись с задачами анализа цепей с высокой точностью, что не оставляло пространства для улучшения.

По итогам экспериментов выбрал следующую модель для дообучения:

 **Qwen2.5-1.5B-instruct**

**Обоснование выбора:**
- Модель способна решать задачи, но справляется не с 100% вероятностью
- Есть пространство для улучшения через обучение на RL среде
- Размер модели позволяет обучение на доступных ресурсах (бесплатный Colab, Kaggle)
- Хорошая производительность на задачах рассуждения

### Проблема с форматом ответов

После выбора модели, я начал ее тестировать на соблюдении формата ответов. По итогам тестирования, я обнаружил, что модель не всегда следует формату из системного промпта.

Это привело к внедрению дополнительной системы оценки для reward функции:

```python
def compute_reward(self, data: Data, response: str) -> float:
    # Базовая оценка точности
    accuracy_score = self.verifier.get_accuracy_score(data, response)
    base_reward = accuracy_score * self.config.reward_scale_factor
    
    # Бонусы за правильный формат
    format_bonus = 0.0
    if self._has_think_tag(response):
        format_bonus += 0.2  # Бонус за <think>...</think> 
    
    if self._has_strict_answer_format(response):
        format_bonus += 0.5  # Бонус за <answer>X.XXX</answer>
    
    return base_reward + format_bonus  # [0, 2.7]
```

**Критерии оценки формата:**
- Наличие тегов `<think>...</think>` - бонус 0.2
- Строгий формат `<answer>X.XXX</answer>` - бонус 0.5
- Общий максимальный бонус за соблюдение формата: 0.7

### Проблема с вычислительными ресурсами

Обучение на бесплатных мощностях (Google Colab, Kaggle) занимало чрезмерно много времени (12+ часов на старой среде), что мешало быстрому получению результатов и итеративной разработке.

Для решения этой проблемы я проанализировал другие бесплатные ресурсы для обучения моделей. В итоге выбрал Colab Pro, для быстроты решения вопроса и обучения модели. 

**Результат:**
- Время обучения сократилось с 6+ часов до 0.5-2 часов (на новой среде, в зависимости от количества задач)
- Получил возможность быстрых итераций и экспериментов


Дальше я настраивал параметры для обучение модели. Для этого я тестировал множество разных параметров и настраивал их под наилучшие результаты.
### Конфигурация обучения

```python
TrainingConfig(
    # Модель
    model_name="unsloth/Qwen2.5-1.5B-instruct",
    max_seq_length=8192,   
    gpu_memory_utilization=0.35, # 35% памяти GPU под vllm 
    
    # LoRA 
    lora_r=64,
    lora_alpha=64,
    lora_dropout=0.1,
    
    # Обучение
    learning_rate=5e-5,
    max_steps=100,
    batch_size=16,
    save_steps=50,
    
    # Dataset
    difficulties=[1, 2, 3, 4, 5, 6],
    samples_per_difficulty=25
)
```

Обучение происходило на 125 задач (6 сложностей по 25 задач, четные задачи это последовательные цепи, нечетные - параллельные) и заняло 30 минут на A100 (40GB)


После чего для оценки обученной модели я реализовал систему оценки:

### Система оценки

Реализована комплексная система оценки:

2. **Baseline model** - базовая модель без специального промпта
3. **GRPO Trained** - обученная модель с LoRA

### Результаты тестирования обученной модели

Провел тестирование на 30 задачах (5 на каждый уровень сложности):



### Основные итоги второго этапа

- Внедрено обучение GRPO на RL-среде
- Проведено большое количество запусков и экспериментов с параметрами
- Получены подробные результаты обучения модели
- Выявлена необходимость доработки методики обучения для предотвращения переобучения на определённых типах цепей


---

## Общие результаты проекта

### Достигнутые улучшения

| Метрика | До | После | Улучшение |
|---------|----|----|-----------|
| **Время обучения** | 12+ часов | 1-2 часа | 6x быстрее |
| **Правильный формат** | 10% | 80% | 8x лучше |
| **Параллельные цепи** | 20% | 40% | 2x лучше |
| **Покрытие тестами** | 0% | 95% | +95% |

### Ключевые выводы

1. **Упрощение архитектуры** - фокус на базовых топологиях позволил быстро создать рабочую систему
2. **Градиентная оценка критична** - бинарная оценка не подходит для RL обучения
3. **Формат ответов имеет значение** - бонусная система кардинально улучшила структурированность
4. **Выбор модели важен** - слишком способные модели не оставляют пространства для улучшения
5. **Вычислительные ресурсы критичны** - A100 ускорил разработку в 6+ раз

### Потенциал развития

1. **Добавление смешанных цепей** - расширение на более сложные топологии
2. **Больше типов вопросов** - мощность, эквивалентное сопротивление
3. **Улучшение reward функции** - более сложные метрики качества
4. **Масштабирование** - обучение на большом количестве задач

---

## Структура проекта

```
tbank_summer_practice/
├── base/                   # Базовые классы
│   ├── data.py             # Data класс
│   ├── verifier.py         # Verifier интерфейс  
│   └── utils.py            # Утилиты + системный промпт
│
├── dc_circuit/             # Основные модули
│   ├── generator.py        # Генератор цепей
│   ├── solver.py           # Решатель цепей
│   ├── verifier.py         # Верификатор с градиентной оценкой
│   ├── prompt.py           # Генерация промптов
│   ├── game.py             # DCCircuitGame
│   └── calculators/        # Калькуляторы ответов
│
├── config/                 # Конфигурации
│   ├── circuit_config.py   # Параметры генерации
│   ├── verifier_config.py  # Параметры верификации
│   └── training_config.py  # Параметры обучения
│
├── training/               # Обучение и оценка
│   ├── rl_trainer.py       # GRPO тренер
│   └── evaluate.py         # Система оценки
│
├── results/                # Результаты экспериментов
│   ├── test_results.json   # Результаты тестирования модели
│   └── *.png               # Графики результатов тестирования
│
├── main.py                 # Демонстрация системы
├── requirements.txt        # Зависимости
└── README.md               # Документация
```

---

## Заключение

> **Я разработал полноценную систему для обучения языковых моделей решению задач анализа электрических цепей.**

---

В ходе проекта удалось:
- Обучить специализированную модель на реальных задачах
- Провести множество экспериментов и добиться ощутимых результатов
- Улучшить структурированность и точность ответов за счёт бонусной системы
- Немного вспомнить физику :)

---

### Самый ценный результат —  
**приобретённый опыт глубокого погружения в работу с языковыми моделями и их обучении в средах с обучением с подкреплением (RL).**

---

<div align="right">

_Мокеев Степан, 2025_ 

</div>