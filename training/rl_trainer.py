"""GRPO –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π.
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dataclasses import dataclass
from typing import List

from torch.utils.data import Dataset
from unsloth import FastLanguageModel
from trl import GRPOTrainer, GRPOConfig

from base.utils import extract_answer
from base.data import Data
from dc_circuit.game import DCCircuitGame
from dc_circuit.verifier import DCCircuitVerifier


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================

@dataclass
class TrainingConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
    
    # –ú–æ–¥–µ–ª—å
    model_name: str = "Qwen/Qwen3-0.6B"
    output_dir: str = "./dc_circuit_model_rl"
    max_seq_length: int = 8192
    
    # LoRA
    lora_r: int = 64
    lora_alpha: int = 64
    lora_dropout: float = 0.05
    
    # –û–±—É—á–µ–Ω–∏–µ
    learning_rate: float = 1e-5
    max_steps: int = 500
    batch_size: int = 4
    gradient_accumulation_steps: int = 2
    num_generations: int = 2
    save_steps: int = 100
    
    # Dataset
    difficulties: List[int] = None
    samples_per_difficulty: int = 500
    
    def __post_init__(self):
        if self.difficulties is None:
            self.difficulties = [1, 2, 3, 4, 5]


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
CONFIG = TrainingConfig()


# ============================================================================
# DATASET
# ============================================================================

class DCCircuitDataset(Dataset):
    """Dataset –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(self, config: TrainingConfig):
        self.game = DCCircuitGame()
        self.config = config
        self._data_cache = None

    def _generate_data(self) -> List[dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ game.generate()"""
        if self._data_cache is None:
            all_data = []
            for difficulty in self.config.difficulties:
                print(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ {difficulty}...")
                data_list = self.game.generate(
                    num_of_questions=self.config.samples_per_difficulty,
                    difficulty=difficulty,
                    max_attempts=30
                )
                print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data_list)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
                all_data.extend([{
                    "prompt": [
                        {"role": "user", "content": f"{data.question}\n<gold>{data.answer}</gold>"}
                    ],
                    "question": data.question,
                    "answer": data.answer,
                    "difficulty": data.difficulty
                } for data in data_list])
            
            self._data_cache = all_data
            print(f"üìä –í—Å–µ–≥–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {len(all_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        return self._data_cache

    def __len__(self) -> int:
        return len(self._generate_data())

    def __getitem__(self, idx: int) -> dict:
        data = self._generate_data()
        return data[idx]


# ============================================================================
# GRPO –¢–†–ï–ù–ï–†
# ============================================================================

class DCCircuitRLTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ DC —Ü–µ–ø–µ–π"""

    def __init__(self, config: TrainingConfig = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–µ—Ä —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        self.config = config or CONFIG
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self._verifier = None
        
        print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GRPO —Ç—Ä–µ–Ω–µ—Ä–∞")
        print(f"   –ú–æ–¥–µ–ª—å: {self.config.model_name}")
        print(f"   –í—ã—Ö–æ–¥: {self.config.output_dir}")
        print(f"   –®–∞–≥–∏: {self.config.max_steps}")

    def setup_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å LoRA."""
        print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å unsloth...")
        
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.model_name,
            max_seq_length=self.config.max_seq_length,
            load_in_4bit=True,
            fast_inference=False
        )
        
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.config.lora_r,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=3407,
        )
        
        self.model.train()
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        self.model.print_trainable_parameters()

    def reward_function(self, prompts, completions, **kwargs) -> List[float]:
        """Reward –Ω–∞ –æ—Å–Ω–æ–≤–µ verifier.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DCCircuitVerifier.get_accuracy_score():
        - 1.0 –∑–∞ –æ—à–∏–±–∫—É <= 0.1% -> reward = 2.0
        - 0.75 –∑–∞ –æ—à–∏–±–∫—É <= 0.2% -> reward = 1.5
        - 0.5 –∑–∞ –æ—à–∏–±–∫—É <= 0.3% -> reward = 1.0
        - 0.25 –∑–∞ –æ—à–∏–±–∫—É <= 0.5% -> reward = 0.5
        - 0.0 –∑–∞ –æ—à–∏–±–∫—É > 0.5% -> reward = 0.0
        """
        if self._verifier is None:
            self._verifier = DCCircuitVerifier()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º completions
        responses = []
        for item in completions:
            if isinstance(item, str):
                responses.append(item)
            elif isinstance(item, dict):
                responses.append(item.get("content", ""))
            elif isinstance(item, list) and item:
                candidate = item[0]
                responses.append(candidate.get("content", "") if isinstance(candidate, dict) else str(candidate))
            else:
                responses.append(str(item))
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —à–∞–≥–∞—Ö
        trainer_state = kwargs.get('trainer_state')
        step = getattr(trainer_state, 'global_step', 0) if trainer_state else 0
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞ —à–∞–≥–∞—Ö: 1, 2, 3, 5, 10, 20, 40, 60, ...
        should_log = (step in [1, 2, 3, 5, 10]) or (step > 10 and step % 20 == 0)
        
        if should_log and prompts and responses:
            print("\n" + "="*80)
            print(f"üìä –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï | Step {step}")
            print("="*80)
            
            prompt_content = prompts[0] if isinstance(prompts[0], str) else \
                           (prompts[0][-1]['content'] if prompts[0] else "")
            gold_start = prompt_content.find("<gold>")
            gold_end = prompt_content.find("</gold>")
            
            if gold_start != -1 and gold_end != -1:
                correct_answer = prompt_content[gold_start + 6:gold_end].strip()
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
                raw_response = responses[0] if responses else ""
                extracted = extract_answer(raw_response) if responses else ""
                
                print(f"\n‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}")
                print(f"\nü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (raw, –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
                print(f"   {raw_response[:200]}...")
                print(f"\nüîç –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: '{extracted}'")
                
                # –í—ã—á–∏—Å–ª—è–µ–º reward –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                if extracted:
                    data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
                    try:
                        score = self._verifier.get_accuracy_score(data, raw_response)
                        reward = score * 2.0
                        print(f"\nüí∞ Accuracy Score: {score:.2f} ‚Üí Reward: {reward:.2f}")
                    except:
                        print(f"\nüí∞ Reward: 0.0 (–æ—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è)")
                else:
                    print(f"\nüí∞ Reward: 0.0 (–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç)")
                
                print("="*80 + "\n")
        
        if not prompts:
            return [0.0] * len(responses)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        prompt_content = prompts[0] if isinstance(prompts[0], str) else \
                        (prompts[0][-1]['content'] if prompts and prompts[0] else "")
        
        gold_start = prompt_content.find("<gold>")
        gold_end = prompt_content.find("</gold>")
        
        if gold_start == -1 or gold_end == -1:
            return [0.0] * len(responses)
        
        correct_answer = prompt_content[gold_start + 6:gold_end].strip()
        
        # –°–æ–∑–¥–∞—ë–º Data –¥–ª—è verifier
        data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
        
        # –í—ã—á–∏—Å–ª—è–µ–º reward —á–µ—Ä–µ–∑ verifier
        rewards = []
        for response in responses:
            try:
                accuracy_score = self._verifier.get_accuracy_score(data, response)
                reward = accuracy_score * 2.0  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–ª—è GRPO
            except Exception:
                reward = 0.0
            rewards.append(reward)
        
        return rewards

    def setup_trainer(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç GRPO —Ç—Ä–µ–Ω–µ—Ä."""
        print("\n  –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GRPO —Ç—Ä–µ–Ω–µ—Ä–∞...")
        
        # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç
        train_dataset = DCCircuitDataset(self.config)
        
        # GRPO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        training_args = GRPOConfig(
            use_vllm=False,
            learning_rate=self.config.learning_rate,
            adam_beta1=0.9,
            adam_beta2=0.99,
            weight_decay=0.1,
            warmup_ratio=0.1,
            lr_scheduler_type="cosine",
            optim="adamw_8bit",
            logging_steps=1,
            per_device_train_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations,
            max_prompt_length=4096,
            max_completion_length=8192,
            max_steps=self.config.max_steps,
            save_steps=self.config.save_steps,
            max_grad_norm=0.1,
            report_to="none",
            output_dir=self.config.output_dir,
            temperature=0.7,
        )
        
        # –°–æ–∑–¥–∞—ë–º —Ç—Ä–µ–Ω–µ—Ä
        self.model.train()
        self.trainer = GRPOTrainer(
            model=self.model,
            processing_class=self.tokenizer,
            reward_funcs=[self.reward_function],
            args=training_args,
            train_dataset=train_dataset,
        )
        
        print("‚úÖ GRPO —Ç—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

    def train(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ."""
        print("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º GRPO –æ–±—É—á–µ–Ω–∏–µ...")
        
        try:
            self.model.train()
            self.trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            self.trainer.save_model(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print(f"üíæ –ú–æ–¥–µ–ª—å: {self.config.output_dir}")
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            checkpoint_dir = f"{self.config.output_dir}/checkpoint"
            os.makedirs(checkpoint_dir, exist_ok=True)
            self.trainer.save_model(checkpoint_dir)
            self.tokenizer.save_pretrained(checkpoint_dir)
            print(f"üíæ Checkpoint: {checkpoint_dir}")
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            raise

    def run(self):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è."""
        self.setup_model()
        self.setup_trainer()
        self.train()


# ============================================================================
# –ó–ê–ü–£–°–ö
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("üéØ DC CIRCUIT GRPO –û–ë–£–ß–ï–ù–ò–ï")
    print("="*80)
    print("\n–¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ñ–∏–≥:")
    print(f"  –ú–æ–¥–µ–ª—å: {CONFIG.model_name}")
    print(f"  –®–∞–≥–∏: {CONFIG.max_steps}")
    print(f"  Batch size: {CONFIG.batch_size}")
    print(f"  Learning rate: {CONFIG.learning_rate}")
    print(f"  –°–ª–æ–∂–Ω–æ—Å—Ç–∏: {CONFIG.difficulties}")
    print(f"  –î–∞—Ç–∞—Å–µ—Ç: {CONFIG.samples_per_difficulty} –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å")
    print("="*80)
    
    trainer = DCCircuitRLTrainer(CONFIG)
    trainer.run()
