"""GRPO –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π.
"""

import os
import sys
import torch
import gc

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ CUDA
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dataclasses import dataclass
from typing import List

from torch.utils.data import Dataset
from unsloth import FastLanguageModel
from trl import GRPOTrainer, GRPOConfig

from base.utils import extract_answer, get_system_prompt
from base.data import Data
from dc_circuit.game import DCCircuitGame
from dc_circuit.verifier import DCCircuitVerifier


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================

@dataclass
class TrainingConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
    
    # –ú–æ–¥–µ–ª—å - –ú–ê–¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
    model_name: str = "unsloth/Qwen3-4B-Instruct-2507"  
    output_dir: str = "./dc_circuit_model_rl"
    max_seq_length: int = 13000  
    
    # LoRA
    lora_r: int = 64  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π rank –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
    lora_alpha: int = 64  # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç r –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
    lora_dropout: float = 0.05
    
    # –û–±—É—á–µ–Ω–∏–µ - –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–ò
    learning_rate: float = 1e-5  # –ù–µ–º–Ω–æ–≥–æ —É–º–µ–Ω—å—à–µ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    max_steps: int = 100  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ RL –æ–±—É—á–µ–Ω–∏—è
    batch_size: int = 2  # –ú–∏–Ω–∏–º—É–º
    gradient_accumulation_steps: int = 2  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (—ç—Ñ—Ñ=2)
    num_generations: int = 4  # –ú–∏–Ω–∏–º—É–º –¥–ª—è GRPO, –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏
    save_steps: int = 25 
    
    # Dataset
    difficulties: List[int] = None
    samples_per_difficulty: int = 100  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ 
    
    def __post_init__(self):
        if self.difficulties is None:
            self.difficulties = [1, 3, 5]  # –ü—Ä–æ—Å—Ç–æ–π, —Å—Ä–µ–¥–Ω–∏–π, —Å–ª–æ–∂–Ω—ã–π


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
CONFIG = TrainingConfig()


# ============================================================================
# DATASET
# ============================================================================

class DCCircuitDataset(Dataset):
    """Dataset –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(self, config: TrainingConfig):
        self.game = DCCircuitGame()
        self.config = config
        self._data_cache = None

    def _generate_data(self) -> List[dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ game.generate()"""
        if self._data_cache is None:
            all_data = []
            for difficulty in self.config.difficulties:
                print(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ {difficulty}...")
                data_list = self.game.generate(
                    num_of_questions=self.config.samples_per_difficulty,
                    difficulty=difficulty,
                    max_attempts=30
                )
                
                all_data.extend([{
                    "prompt": [
                        {"role": "system", "content": get_system_prompt()},
                        {"role": "user", "content": f"{data.question}\n<gold>{float(data.answer):.3f}</gold>"}
                    ],
                    "question": data.question,
                    "answer": f"{float(data.answer):.3f}",
                    "difficulty": data.difficulty
                } for data in data_list])
            
            self._data_cache = all_data
        
        return self._data_cache

    def __len__(self) -> int:
        return len(self._generate_data())

    def __getitem__(self, idx: int) -> dict:
        data = self._generate_data()
        return data[idx]


# ============================================================================
# GRPO –¢–†–ï–ù–ï–†
# ============================================================================

class DCCircuitRLTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ DC —Ü–µ–ø–µ–π"""

    def __init__(self, config: TrainingConfig = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–µ—Ä —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        self.config = config or CONFIG
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self._verifier = None

    def setup_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å LoRA"""        
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.config.model_name}...")
        
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.model_name,
            max_seq_length=self.config.max_seq_length,
            load_in_4bit=True,  
            dtype=None,  # Auto
            fast_inference=True,
            gpu_memory_utilization=0.23  
        )
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ chat_template –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ message['content'] }}{% endif %}{% endfor %}"
        
        # LoRA –æ–±—É—á–µ–Ω–∏–µ
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.config.lora_r,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=3407,
        )
        
        self.model.train()
        self.model.print_trainable_parameters()

    def _extract_prompt_content(self, prompts) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.
        
        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å str –∏–ª–∏ list of dicts)
        
        Returns:
            –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
        """
        if not prompts:
            return ""
        if isinstance(prompts[0], str):
            return prompts[0]
        return prompts[0][-1]['content'] if prompts[0] else ""
    
    def _normalize_completions(self, completions) -> List[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç completions –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
        
        Args:
            completions: –û—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        """
        responses = []
        for item in completions:
            if isinstance(item, str):
                responses.append(item)
            elif isinstance(item, dict):
                responses.append(item.get("content", ""))
            elif isinstance(item, list) and item:
                candidate = item[0]
                responses.append(candidate.get("content", "") if isinstance(candidate, dict) else str(candidate))
            else:
                responses.append(str(item))
        return responses
    
    def _should_log_step(self, step: int) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—É—â–∏–π —à–∞–≥.
        
        Args:
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞
        
        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å
        """
        return (step in [1, 2, 3, 5, 10]) or (step > 10 and step % 20 == 0)
    
    def _extract_gold_answer(self, prompt_content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt_content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞
        
        Returns:
            –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        gold_start = prompt_content.find("<gold>")
        gold_end = prompt_content.find("</gold>")
        
        if gold_start != -1 and gold_end != -1:
            return prompt_content[gold_start + 6:gold_end].strip()
        return ""
    
    def _log_detailed_metrics(self, step: int, correct_answer: str,
                             raw_response: str, extracted: str):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫.

        Args:
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞
            correct_answer: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            raw_response: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
            extracted: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
        with open("training_detailed_log.txt", "a", encoding="utf-8") as f:
            f.write(f"\n{'='*80}\n")
            f.write(f"üìä –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï | Step {step}\n")
            f.write(f"{'='*80}\n")
            f.write(f"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}\n")
            f.write(f"ü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (–ø–æ–ª–Ω—ã–π):\n{raw_response}\n")
            f.write(f"üîç –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: '{extracted}'\n")

        print("\n" + "="*80)
        print(f"üìä –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï | Step {step}")
        print("="*80)
        print(f"\n‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}")
        print(f"\nü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (raw, –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
        print(f"   {raw_response[:200]}...")
        print(f"\nüîç –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: '{extracted}'")
        
        # –í—ã—á–∏—Å–ª—è–µ–º reward –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        if extracted:
            data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
            try:
                score = self._verifier.get_accuracy_score(data, raw_response)
                reward = score * 2.0
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏
                try:
                    model_val = float(extracted)
                    correct_val = float(correct_answer)
                    rounded_correct = round(correct_val, 3)
                    rounded_model = round(model_val, 3)
                    
                    if abs(rounded_correct) < 1e-12:
                        rel_error = abs(rounded_model - rounded_correct)
                        rel_error_percent = rel_error * 100
                    else:
                        rel_error = abs(rounded_model - rounded_correct) / abs(rounded_correct)
                        rel_error_percent = rel_error * 100
                    
                    print(f"\nüî¨ –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç:")
                    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ (raw):       {correct_val}")
                    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ (–æ–∫—Ä—É–≥–ª.):   {rounded_correct}")
                    print(f"   –ú–æ–¥–µ–ª—å (raw):          {model_val}")
                    print(f"   –ú–æ–¥–µ–ª—å (–æ–∫—Ä—É–≥–ª.):      {rounded_model}")
                    print(f"   –ê–±—Å. –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å:      {abs(rounded_model - rounded_correct):.6f}")
                    print(f"   –û—Ç–Ω. –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å:      {rel_error:.6f} ({rel_error_percent:.2f}%)")
                    print(f"   –ü–æ—Ä–æ–≥–∏: 1%={0.01}, 5%={0.05}, 10%={0.10}, 20%={0.20}")
                except:
                    pass
                
                print(f"\nüí∞ Accuracy Score: {score:.2f} ‚Üí Reward: {reward:.2f}")
            except Exception as e:
                print(f"\nüí∞ Reward: 0.0 (–æ—à–∏–±–∫–∞: {e})")
        else:
            print(f"\nüí∞ Reward: 0.0 (–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç)")

    def _log_detailed_metrics_for_generation(self, step: int, generation_num: int, correct_answer: str,
                                           raw_response: str, extracted: str):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

        Args:
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞
            generation_num: –ù–æ–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (1-4)
            correct_answer: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            raw_response: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
            extracted: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
        with open("training_detailed_log.txt", "a", encoding="utf-8") as f:
            f.write(f"\n{'='*60}\n")
            f.write(f"üìä Step {step} | Generation {generation_num}\n")
            f.write(f"{'='*60}\n")
            f.write(f"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}\n")
            f.write(f"ü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (–ø–æ–ª–Ω—ã–π):\n{raw_response}\n")
            f.write(f"üîç –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: '{extracted}'\n")

            # –í—ã—á–∏—Å–ª—è–µ–º reward –¥–ª—è —ç—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if extracted:
                data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
                try:
                    score = self._verifier.get_accuracy_score(data, raw_response)
                    reward = score * 2.0

                    # –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏
                    try:
                        model_val = float(extracted)
                        correct_val = float(correct_answer)
                        rounded_correct = round(correct_val, 3)
                        rounded_model = round(model_val, 3)

                        if abs(rounded_correct) < 1e-12:
                            rel_error = abs(rounded_model - rounded_correct)
                            rel_error_percent = rel_error * 100
                        else:
                            rel_error = abs(rounded_model - rounded_correct) / abs(rounded_correct)
                            rel_error_percent = rel_error * 100

                        f.write(f"üî¨ –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç:\n")
                        f.write(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ (–æ–∫—Ä—É–≥–ª.):   {rounded_correct}\n")
                        f.write(f"   –ú–æ–¥–µ–ª—å (–æ–∫—Ä—É–≥–ª.):      {rounded_model}\n")
                        f.write(f"   –û—Ç–Ω. –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å:      {rel_error:.6f} ({rel_error_percent:.2f}%)\n")
                        f.write(f"üí∞ Accuracy Score: {score:.2f} ‚Üí Reward: {reward:.2f}\n")
                    except:
                        f.write(f"üí∞ Accuracy Score: {score:.2f} ‚Üí Reward: {reward:.2f}\n")
                except Exception as e:
                    f.write(f"üí∞ Reward: 0.0 (–æ—à–∏–±–∫–∞: {e})\n")
            else:
                f.write(f"üí∞ Reward: 0.0 (–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç)\n")

        print(f"\nüîÑ Generation {generation_num}:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π: {correct_answer} | –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π: '{extracted}'")

    def _calculate_rewards(self, correct_answer: str, responses: List[str]) -> List[float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç rewards –¥–ª—è –≤—Å–µ—Ö –æ—Ç–≤–µ—Ç–æ–≤.
        
        Args:
            correct_answer: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            responses: –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ rewards
        """
        data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
        rewards = []
        
        for response in responses:
            try:
                accuracy_score = self._verifier.get_accuracy_score(data, response)
                reward = accuracy_score * 2.0
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ _calculate_rewards: {e}")
                reward = 0.0
            rewards.append(reward)
        
        return rewards
    
    def reward_function(self, prompts, completions, **kwargs) -> List[float]:
        """Reward –Ω–∞ –æ—Å–Ω–æ–≤–µ verifier.

        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
            completions: –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (trainer_state)
        
        Returns:
            –°–ø–∏—Å–æ–∫ rewards –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è verifier
        if self._verifier is None:
            self._verifier = DCCircuitVerifier()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
        responses = self._normalize_completions(completions)
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —à–∞–≥–∞—Ö
        trainer_state = kwargs.get('trainer_state')
        step = getattr(trainer_state, 'global_step', 0) if trainer_state else 0
        
        if self._should_log_step(step) and prompts and responses:
            prompt_content = self._extract_prompt_content(prompts)
            correct_answer = self._extract_gold_answer(prompt_content)

            if correct_answer:
                # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ 4 –æ—Ç–≤–µ—Ç–∞
                for i, raw_response in enumerate(responses):
                    extracted = extract_answer(raw_response)
                    self._log_detailed_metrics_for_generation(step, i+1, correct_answer, raw_response, extracted)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏ –≤—ã—á–∏—Å–ª—è–µ–º rewards
        prompt_content = self._extract_prompt_content(prompts)
        correct_answer = self._extract_gold_answer(prompt_content)
        
        if not correct_answer:
            return [0.0] * len(responses)

        rewards = self._calculate_rewards(correct_answer, responses)

        # –û—Ç–ª–∞–¥–∫–∞: –≤—ã–≤–æ–¥–∏–º rewards –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—á–µ–º—É loss=0
        if self._should_log_step(getattr(kwargs.get('trainer_state'), 'global_step', 0)):
            print(f"üîç Rewards: {rewards}")
            print(f"üîç Mean reward: {sum(rewards)/len(rewards):.4f}")

        return rewards

    def setup_trainer(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç GRPO —Ç—Ä–µ–Ω–µ—Ä."""
        train_dataset = DCCircuitDataset(self.config)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU –¥–ª—è DDP

        
        training_args = GRPOConfig(
            use_vllm=True,  # –í–∫–ª—é—á–∞–µ–º vLLM –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            learning_rate=self.config.learning_rate,
            adam_beta1=0.9,
            adam_beta2=0.99,
            weight_decay=0.1,
            warmup_ratio=0.1,
            lr_scheduler_type="cosine",
            optim="adamw_8bit",
            logging_steps=1,
            per_device_train_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations,
            max_prompt_length=4096,  # –ü–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            max_completion_length=10000,  # –ü–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            max_steps=self.config.max_steps,
            save_steps=self.config.save_steps,
            max_grad_norm=0.1,
            report_to="none",
            output_dir=self.config.output_dir,
            temperature=0.7,
            repetition_penalty=1.1,

        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
        self.model.train()
        self.trainer = GRPOTrainer(
            model=self.model,
            processing_class=self.tokenizer,
            reward_funcs=[self.reward_function],
            args=training_args,
            train_dataset=train_dataset,
        )
        

    def train(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ."""
        try:
            num_gpus = torch.cuda.device_count()
            
            self.model.train()
            self.trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.trainer.save_model(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
        except KeyboardInterrupt:
            checkpoint_dir = f"{self.config.output_dir}/checkpoint"
            os.makedirs(checkpoint_dir, exist_ok=True)
            self.trainer.save_model(checkpoint_dir)
            self.tokenizer.save_pretrained(checkpoint_dir)
        except Exception as e:
            raise
        finally:
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'trainer') and self.trainer is not None:
                del self.trainer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

    def run(self):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è."""
        print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è GRPO")

        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        with open("training_detailed_log.txt", "w", encoding="utf-8") as f:
            f.write("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø GRPO\n")
            f.write(f"–í—Ä–µ–º—è: {torch.__version__}\n")
            f.write(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: batch_size={self.config.batch_size}, num_generations={self.config.num_generations}\n")
            f.write("="*80 + "\n\n")

        self.setup_model()
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        self.setup_trainer()
        print("‚úÖ –¢—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        self.train()

if __name__ == "__main__":    
    trainer = DCCircuitRLTrainer(CONFIG)
    trainer.run()