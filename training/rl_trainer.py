"""GRPO –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π.
"""

import os
import sys
import torch
import json
import datetime
import re


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import List

from torch.utils.data import Dataset
from unsloth import FastLanguageModel
from trl import GRPOTrainer, GRPOConfig

from base.utils import get_system_prompt
from base.data import Data
from dc_circuit.game import DCCircuitGame
from dc_circuit.verifier import DCCircuitVerifier
from config import TrainingConfig, CircuitConfig, VerifierConfig 

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
CONFIG = TrainingConfig()


class DCCircuitDataset(Dataset):
    """Dataset –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(self, config: TrainingConfig, circuit_config: CircuitConfig = None, verifier_config: VerifierConfig = None):
        circuit_config = circuit_config or CircuitConfig()
        verifier_config = verifier_config or VerifierConfig()
        self.game = DCCircuitGame(circuit_config, verifier_config)
        self.config = config
        self.data = None  # –ö—ç—à–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        self._generate_data()

    def _generate_data(self) -> None:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ game.generate() (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏)"""
        if self.data is not None:
            return  # –£–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ
            
        all_data = []
        for difficulty in self.config.difficulties:
            data_list = self.game.generate(
                num_of_questions=self.config.samples_per_difficulty,
                difficulty=difficulty
            )
            
            all_data.extend([{
                "prompt": [
                {"role": "system", "content": get_system_prompt()},
                {"role": "user", "content": data.question}
                ],
                "question": data.question,
                "answer": f"{float(data.answer):.3f}",
                "difficulty": data.difficulty
            } for data in data_list])
        
        self.data = all_data

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> dict:
        return self.data[idx]


class DCCircuitRLTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ DC —Ü–µ–ø–µ–π"""
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    REWARD_SCALE_FACTOR = 2.0
    FORMAT_BONUS = 0.2  # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞
    STRICT_FORMAT_BONUS = 0.5  # –ë–æ–Ω—É—Å –∑–∞ —Å—Ç—Ä–æ–≥–∏–π —Ñ–æ—Ä–º–∞—Ç <answer>X.XXX</answer>
    RANDOM_STATE = 3407

    def __init__(self, config: TrainingConfig = None, circuit_config: CircuitConfig = None, verifier_config: VerifierConfig = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–µ—Ä —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        self.config = config or CONFIG
        self.circuit_config = circuit_config or CircuitConfig()
        self.verifier_config = verifier_config or VerifierConfig()
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self._verifier = None
        self.dataset = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º dataset –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–∏
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è LLM
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.llm_log_file = f"llm_logs_{timestamp}.jsonl"
        self.log_entries = []

    def setup_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å LoRA"""        
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.model_name,
            max_seq_length=self.config.max_seq_length,
            load_in_4bit=self.config.use_4bit,  
            dtype=self.config.dtype,
            fast_inference=True,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            use_flash_attention=self.config.use_flash_attention,
            device_map="auto"  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU
        )
        
        if self.tokenizer.chat_template is None:
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π chat template –¥–ª—è Qwen3
            self.tokenizer.chat_template = """{% for message in messages %}
{% if message['role'] == 'system' %}
<|im_start|>system
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'user' %}
<|im_start|>user
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'assistant' %}
<|im_start|>assistant
{{ message['content'] }}<|im_end|>
{% endif %}
{% endfor %}"""
        
        # LoRA –æ–±—É—á–µ–Ω–∏–µ
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.config.lora_r,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=self.RANDOM_STATE,
        )
        
        self.model.train()
        self.model.print_trainable_parameters()

    def log_llm_interaction(self, prompt, completion, reward=None, metadata=None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å LLM"""
        entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "prompt": prompt,
            "completion": completion,
            "reward": reward,
            "metadata": metadata or {}
        }
        self.log_entries.append(entry)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
        with open(self.llm_log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')

    def save_llm_logs(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –ª–æ–≥–∏ LLM"""
        with open(f"llm_logs_complete_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
            json.dump(self.log_entries, f, ensure_ascii=False, indent=2)
    
    def reward_function(self, prompts, completions, **kwargs) -> List[float]:
        """Reward –Ω–∞ –æ—Å–Ω–æ–≤–µ verifier.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ dataset.
        """
        if self._verifier is None:
            self._verifier = DCCircuitVerifier(self.verifier_config)
        
        if self.dataset is None:
            raise ValueError("dataset not initialized")
        
        # –í—ã—á–∏—Å–ª—è–µ–º rewards –¥–ª—è –∫–∞–∂–¥–æ–≥–æ completion
        rewards = []
        for idx, completion in enumerate(completions):
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –≤ dataset –ø–æ –ø—Ä–æ–º–ø—Ç—É
            prompt_text = prompts[idx] if isinstance(prompts[idx], str) else str(prompts[idx])
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –ø—Ä–æ–º–ø—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ user)
            correct_answer = None
            question_from_prompt = None
            
            # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            if isinstance(prompts[idx], list):
                for msg in reversed(prompts[idx]):
                    if isinstance(msg, dict) and msg.get('role') == 'user':
                        question_from_prompt = msg.get('content', '')
                        break
            else:
                # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç - —Å—Ç—Ä–æ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë —Ü–µ–ª–∏–∫–æ–º
                question_from_prompt = prompt_text
            
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–¥–∞—á—É –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –ü–û –°–û–î–ï–†–ñ–ò–ú–û–ú–£
            if question_from_prompt:
                for data_item in self.dataset:
                    if data_item["question"] == question_from_prompt:
                        correct_answer = data_item["answer"]
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –≤–æ–ø—Ä–æ—Å–∞: {correct_answer}")
                        break
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –∏—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ
                if correct_answer is None:
                    for data_item in self.dataset:
                        if data_item["question"] in question_from_prompt or question_from_prompt in data_item["question"]:
                            correct_answer = data_item["answer"]
                            print(f"‚úÖ –ù–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é: {correct_answer}")
                            break
            
            accuracy_score = None
            reward = 0.0
            
            if correct_answer is None:
                print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {idx}")
            else:
                data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
                completion_str_for_verifier = str(completion) if not isinstance(completion, str) else completion
                accuracy_score = self._verifier.get_accuracy_score(data, completion_str_for_verifier)
                
                # –ë–∞–∑–æ–≤—ã–π reward –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å
                base_reward = accuracy_score * self.REWARD_SCALE_FACTOR
                
                # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                format_bonus = 0.0
                strict_format_bonus = 0.0
                has_think_tag = "<think>" in completion_str_for_verifier
                has_answer_tag = "<answer>" in completion_str_for_verifier
                
                # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –≤–Ω—É—Ç—Ä–∏ <answer> —Ä–æ–≤–Ω–æ —á–∏—Å–ª–æ —Å 3 –∑–Ω–∞–∫–∞–º–∏ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏
                strict_format_ok = False
                try:
                    answer_tags = re.findall(r"<answer>([\s\S]*?)</answer>", completion_str_for_verifier, flags=re.IGNORECASE)
                    if answer_tags:
                        last_answer = answer_tags[-1].strip()
                        strict_format_ok = bool(re.fullmatch(r"[-+]?\d+\.\d{3}", last_answer))
                except Exception:
                    strict_format_ok = False
                
                if has_think_tag and has_answer_tag:
                    format_bonus = self.FORMAT_BONUS
                    print(f"üéØ –ë–æ–Ω—É—Å –∑–∞ —Ñ–æ—Ä–º–∞—Ç: +{format_bonus:.1f}")
                if strict_format_ok:
                    strict_format_bonus = self.STRICT_FORMAT_BONUS
                    print(f"üîí –ë–æ–Ω—É—Å –∑–∞ —Å—Ç—Ä–æ–≥–∏–π —Ñ–æ—Ä–º–∞—Ç: +{strict_format_bonus:.1f}")
                
                reward = base_reward + format_bonus + strict_format_bonus
                print(f"‚úÖ Accuracy: {accuracy_score:.3f}, base_reward: {base_reward:.3f}, total_reward: {reward:.3f}")
            
            self.log_llm_interaction(
                prompt=prompt_text,
                completion=completion,
                reward=reward,
                metadata={
                    "correct_answer": correct_answer,
                    "accuracy_score": accuracy_score if correct_answer else None,
                    "base_reward": base_reward if correct_answer else 0.0,
                    "format_bonus": format_bonus,
                        "strict_format_bonus": strict_format_bonus,
                    "total_reward": reward,
                    "batch_idx": idx,
                    "completion_has_think": has_think_tag,
                    "completion_has_answer": has_answer_tag,
                        "has_correct_format": has_think_tag and has_answer_tag,
                        "has_strict_answer_format": strict_format_ok
                }
            )
            
            rewards.append(reward)

        return rewards

    def setup_trainer(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ GRPO —Ç—Ä–µ–Ω–µ—Ä–∞."""
        train_dataset = DCCircuitDataset(self.config, self.circuit_config, self.verifier_config)
        self.dataset = train_dataset  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–∏
        
        training_args = GRPOConfig(
            use_vllm=True, 
            learning_rate=self.config.learning_rate,
            adam_beta1=self.config.adam_beta1,
            adam_beta2=self.config.adam_beta2,
            weight_decay=self.config.weight_decay,
            warmup_ratio=self.config.warmup_ratio,
            lr_scheduler_type="cosine",
            optim="adamw_8bit",
            logging_steps=1,
            per_device_train_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations,
            max_prompt_length=4096, 
            max_completion_length=self.config.max_completion_length,  
            max_steps=self.config.max_steps,
            save_steps=self.config.save_steps,
            max_grad_norm=self.config.max_grad_norm,
            report_to="none",
            output_dir=self.config.output_dir,
            temperature=self.config.temperature,
            repetition_penalty=self.config.repetition_penalty,
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ GRPO
        self.model.train()
        self.trainer = GRPOTrainer(
            model=self.model,
            processing_class=self.tokenizer,
            reward_funcs=[self.reward_function],
            args=training_args,
            train_dataset=train_dataset,
        )
        

    def train(self):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è."""
        try:        
            self.model.train()
            self.trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ GRPO
            self.trainer.save_model(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ LLM
            self.save_llm_logs()
            print(f"‚úÖ –õ–æ–≥–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.llm_log_file}")
            
        except KeyboardInterrupt:
            checkpoint_dir = f"{self.config.output_dir}/checkpoint"
            os.makedirs(checkpoint_dir, exist_ok=True)
            self.trainer.save_model(checkpoint_dir)
            self.tokenizer.save_pretrained(checkpoint_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –ø—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏
            self.save_llm_logs()
            print(f"‚úÖ –õ–æ–≥–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.llm_log_file}")
        except Exception as e:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.save_llm_logs()
            print(f"‚úÖ –õ–æ–≥–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.llm_log_file}")
            raise

    def run(self):
        """–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è GRPO."""
        self.setup_model()
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        self.setup_trainer()
        print("‚úÖ –¢—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        self.train()
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

if __name__ == "__main__":    
    trainer = DCCircuitRLTrainer(CONFIG)
    trainer.run()