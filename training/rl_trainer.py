"""GRPO –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π.
"""

import os
import sys

# ============================================================================
# vLLM –û–¢–ö–õ–Æ–ß–ï–ù –î–õ–Ø T4 15GB - –°–õ–ò–®–ö–û–ú –ú–ù–û–ì–û –ü–ê–ú–Ø–¢–ò!
# Unsloth –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è, –ø–æ—ç—Ç–æ–º—É –±–µ–∑ vLLM —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ!
# ============================================================================

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dataclasses import dataclass
from typing import List

from torch.utils.data import Dataset
from unsloth import FastLanguageModel
from trl import GRPOTrainer, GRPOConfig

from base.utils import extract_answer
from base.data import Data
from dc_circuit.game import DCCircuitGame
from dc_circuit.verifier import DCCircuitVerifier


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================

@dataclass
class TrainingConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
    
    # –ú–æ–¥–µ–ª—å
    model_name: str = "unsloth/Qwen2.5-3B-Instruct"
    output_dir: str = "./dc_circuit_model_rl"
    max_seq_length: int = 2048  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è KV cache!
    
    # LoRA
    lora_r: int = 64
    lora_alpha: int = 64
    lora_dropout: float = 0.05
    
    # –û–±—É—á–µ–Ω–∏–µ
    learning_rate: float = 1e-5
    max_steps: int = 50  # –ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è T4
    batch_size: int = 1  
    gradient_accumulation_steps: int = 24  # –ö–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ–º (—ç—Ñ—Ñ=24)
    num_generations: int = 1  # –î–ª—è vLLM —Ç–æ–ª—å–∫–æ 1! 
    save_steps: int = 25 
    
    # Dataset (–£–í–ï–õ–ò–ß–ò–õ–ò –î–õ–Ø –õ–£–ß–®–ï–ì–û –û–ë–£–ß–ï–ù–ò–Ø)
    difficulties: List[int] = None
    samples_per_difficulty: int = 25  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 50 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏!
    
    def __post_init__(self):
        if self.difficulties is None:
            self.difficulties = [1, 3, 5]  # –ü—Ä–æ—Å—Ç–æ–π, —Å—Ä–µ–¥–Ω–∏–π, —Å–ª–æ–∂–Ω—ã–π - –≤–µ—Å—å –¥–∏–∞–ø–∞–∑–æ–Ω!


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
CONFIG = TrainingConfig()


# ============================================================================
# DATASET
# ============================================================================

class DCCircuitDataset(Dataset):
    """Dataset –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(self, config: TrainingConfig):
        self.game = DCCircuitGame()
        self.config = config
        self._data_cache = None

    def _generate_data(self) -> List[dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ game.generate()"""
        if self._data_cache is None:
            all_data = []
            for difficulty in self.config.difficulties:
                print(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ {difficulty}...")
                data_list = self.game.generate(
                    num_of_questions=self.config.samples_per_difficulty,
                    difficulty=difficulty,
                    max_attempts=30
                )
                print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data_list)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
                all_data.extend([{
                    "prompt": [
                        {"role": "system", "content": "You are an expert in DC circuit analysis. IMPORTANT: Show ALL your calculations and reasoning steps, but at the very end, provide the FINAL ANSWER in this exact format: 'The answer is: [numerical_value]'. Do not include units in the final answer format, only the number."},
                        {"role": "user", "content": f"{data.question}\n<gold>{float(data.answer):.3f}</gold>"}
                    ],
                    "question": data.question,
                    "answer": f"{float(data.answer):.3f}",
                    "difficulty": data.difficulty
                } for data in data_list])
            
            self._data_cache = all_data
            print(f"üìä –í—Å–µ–≥–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {len(all_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        return self._data_cache

    def __len__(self) -> int:
        return len(self._generate_data())

    def __getitem__(self, idx: int) -> dict:
        data = self._generate_data()
        return data[idx]


# ============================================================================
# GRPO –¢–†–ï–ù–ï–†
# ============================================================================

class DCCircuitRLTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ DC —Ü–µ–ø–µ–π"""

    def __init__(self, config: TrainingConfig = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–µ—Ä —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        self.config = config or CONFIG
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self._verifier = None

    def setup_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å LoRA"""        
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.config.model_name}...")
        
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.model_name,
            max_seq_length=self.config.max_seq_length,
            load_in_4bit=True,  
            dtype=None,  # Auto
            fast_inference=True,  # vLLM –í–ö–õ–Æ–ß–ï–ù!
            max_lora_rank=self.config.lora_r, 
            gpu_memory_utilization=0.5  
        )
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ chat_template –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ message['content'] }}{% endif %}{% endfor %}"
        
        # LoRA –æ–±—É—á–µ–Ω–∏–µ
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.config.lora_r,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=3407,
        )
        
        self.model.train()
        self.model.print_trainable_parameters()

    def _extract_prompt_content(self, prompts) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.
        
        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å str –∏–ª–∏ list of dicts)
        
        Returns:
            –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
        """
        if not prompts:
            return ""
        if isinstance(prompts[0], str):
            return prompts[0]
        return prompts[0][-1]['content'] if prompts[0] else ""
    
    def _normalize_completions(self, completions) -> List[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç completions –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
        
        Args:
            completions: –û—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        """
        responses = []
        for item in completions:
            if isinstance(item, str):
                responses.append(item)
            elif isinstance(item, dict):
                responses.append(item.get("content", ""))
            elif isinstance(item, list) and item:
                candidate = item[0]
                responses.append(candidate.get("content", "") if isinstance(candidate, dict) else str(candidate))
            else:
                responses.append(str(item))
        return responses
    
    def _should_log_step(self, step: int) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—É—â–∏–π —à–∞–≥.
        
        Args:
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞
        
        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å
        """
        return (step in [1, 2, 3, 5, 10]) or (step > 10 and step % 20 == 0)
    
    def _extract_gold_answer(self, prompt_content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt_content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞
        
        Returns:
            –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        gold_start = prompt_content.find("<gold>")
        gold_end = prompt_content.find("</gold>")
        
        if gold_start != -1 and gold_end != -1:
            return prompt_content[gold_start + 6:gold_end].strip()
        return ""
    
    def _log_detailed_metrics(self, step: int, correct_answer: str, 
                             raw_response: str, extracted: str):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫.
        
        Args:
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞
            correct_answer: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            raw_response: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
            extracted: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        print("\n" + "="*80)
        print(f"üìä –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï | Step {step}")
        print("="*80)
        print(f"\n‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}")
        print(f"\nü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (raw, –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
        print(f"   {raw_response[:200]}...")
        print(f"\nüîç –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: '{extracted}'")
        
        # –í—ã—á–∏—Å–ª—è–µ–º reward –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        if extracted:
            data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
            try:
                score = self._verifier.get_accuracy_score(data, raw_response)
                reward = score * 2.0
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏
                try:
                    model_val = float(extracted)
                    correct_val = float(correct_answer)
                    rounded_correct = round(correct_val, 3)
                    rounded_model = round(model_val, 3)
                    
                    if abs(rounded_correct) < 1e-12:
                        rel_error = abs(rounded_model - rounded_correct)
                        rel_error_percent = rel_error * 100
                    else:
                        rel_error = abs(rounded_model - rounded_correct) / abs(rounded_correct)
                        rel_error_percent = rel_error * 100
                    
                    print(f"\nüî¨ –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç:")
                    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ (raw):       {correct_val}")
                    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ (–æ–∫—Ä—É–≥–ª.):   {rounded_correct}")
                    print(f"   –ú–æ–¥–µ–ª—å (raw):          {model_val}")
                    print(f"   –ú–æ–¥–µ–ª—å (–æ–∫—Ä—É–≥–ª.):      {rounded_model}")
                    print(f"   –ê–±—Å. –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å:      {abs(rounded_model - rounded_correct):.6f}")
                    print(f"   –û—Ç–Ω. –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å:      {rel_error:.6f} ({rel_error_percent:.2f}%)")
                    print(f"   –ü–æ—Ä–æ–≥–∏: 0.1%={0.001}, 0.2%={0.002}, 0.3%={0.003}, 0.5%={0.005}")
                except:
                    pass
                
                print(f"\nüí∞ Accuracy Score: {score:.2f} ‚Üí Reward: {reward:.2f}")
            except Exception as e:
                print(f"\nüí∞ Reward: 0.0 (–æ—à–∏–±–∫–∞: {e})")
        else:
            print(f"\nüí∞ Reward: 0.0 (–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç)")
    
    
    def _calculate_rewards(self, correct_answer: str, responses: List[str]) -> List[float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç rewards –¥–ª—è –≤—Å–µ—Ö –æ—Ç–≤–µ—Ç–æ–≤.
        
        Args:
            correct_answer: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            responses: –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ rewards
        """
        data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
        rewards = []
        
        for response in responses:
            try:
                accuracy_score = self._verifier.get_accuracy_score(data, response)
                reward = accuracy_score * 2.0
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ _calculate_rewards: {e}")
                reward = 0.0
            rewards.append(reward)
        
        return rewards
    
    def reward_function(self, prompts, completions, **kwargs) -> List[float]:
        """Reward –Ω–∞ –æ—Å–Ω–æ–≤–µ verifier.

        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
            completions: –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (trainer_state)
        
        Returns:
            –°–ø–∏—Å–æ–∫ rewards –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è verifier
        if self._verifier is None:
            self._verifier = DCCircuitVerifier()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
        responses = self._normalize_completions(completions)
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —à–∞–≥–∞—Ö
        trainer_state = kwargs.get('trainer_state')
        step = getattr(trainer_state, 'global_step', 0) if trainer_state else 0
        
        if self._should_log_step(step) and prompts and responses:
            prompt_content = self._extract_prompt_content(prompts)
            correct_answer = self._extract_gold_answer(prompt_content)
            
            if correct_answer:
                raw_response = responses[0] if responses else ""
                extracted = extract_answer(raw_response) if responses else ""
                self._log_detailed_metrics(step, correct_answer, raw_response, extracted)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏ –≤—ã—á–∏—Å–ª—è–µ–º rewards
        prompt_content = self._extract_prompt_content(prompts)
        correct_answer = self._extract_gold_answer(prompt_content)
        
        if not correct_answer:
            return [0.0] * len(responses)
        
        return self._calculate_rewards(correct_answer, responses)

    def setup_trainer(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç GRPO —Ç—Ä–µ–Ω–µ—Ä."""
        train_dataset = DCCircuitDataset(self.config)
        
        training_args = GRPOConfig(
            use_vllm=True,  # –í–ö–õ–Æ–ß–ò–õ–ò! 
            learning_rate=self.config.learning_rate,
            adam_beta1=0.9,
            adam_beta2=0.99,
            weight_decay=0.1,
            warmup_ratio=0.1,
            lr_scheduler_type="cosine",
            optim="adamw_8bit",
            logging_steps=1,
            per_device_train_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations,
            max_prompt_length=1024,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –ø–∞–º—è—Ç–∏!
            max_completion_length=1024,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –ø–∞–º—è—Ç–∏!
            max_steps=self.config.max_steps,
            save_steps=self.config.save_steps,
            max_grad_norm=0.1,
            report_to="none",
            output_dir=self.config.output_dir,
            temperature=0.7,
            repetition_penalty=1.1,
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
        self.model.train()
        self.trainer = GRPOTrainer(
            model=self.model,
            processing_class=self.tokenizer,
            reward_funcs=[self.reward_function],
            args=training_args,
            train_dataset=train_dataset,
        )
        

    def train(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ."""
        try:
            self.model.train()
            self.trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.trainer.save_model(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
        except KeyboardInterrupt:
            checkpoint_dir = f"{self.config.output_dir}/checkpoint"
            os.makedirs(checkpoint_dir, exist_ok=True)
            self.trainer.save_model(checkpoint_dir)
            self.tokenizer.save_pretrained(checkpoint_dir)
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            raise

    def run(self):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è."""
        self.setup_model()
        self.setup_trainer()
        self.train()

if __name__ == "__main__":    
    trainer = DCCircuitRLTrainer(CONFIG)
    trainer.run()