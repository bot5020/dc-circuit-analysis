"""GRPO –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π.
"""

import os
import sys
import torch
import json
import datetime


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import List

from torch.utils.data import Dataset
from unsloth import FastLanguageModel
from trl import GRPOTrainer, GRPOConfig

from base.utils import get_system_prompt
from base.data import Data
from dc_circuit.game import DCCircuitGame
from dc_circuit.verifier import DCCircuitVerifier
from config import TrainingConfig, CircuitConfig, VerifierConfig 

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
CONFIG = TrainingConfig()


class DCCircuitDataset(Dataset):
    """Dataset –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(self, config: TrainingConfig, circuit_config: CircuitConfig = None, verifier_config: VerifierConfig = None):
        circuit_config = circuit_config or CircuitConfig()
        verifier_config = verifier_config or VerifierConfig()
        self.game = DCCircuitGame(circuit_config, verifier_config)
        self.config = config
        self.data = None  # –ö—ç—à–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        self._generate_data()

    def _generate_data(self) -> None:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ game.generate() (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏)"""
        if self.data is not None:
            return  # –£–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ
            
        all_data = []
        for difficulty in self.config.difficulties:
            data_list = self.game.generate(
                num_of_questions=self.config.samples_per_difficulty,
                difficulty=difficulty
            )
            
            all_data.extend([{
                "prompt": [
                {"role": "system", "content": get_system_prompt()},
                {"role": "user", "content": data.question}
                ],
                "question": data.question,
                "answer": f"{float(data.answer):.3f}",
                "difficulty": data.difficulty
            } for data in data_list])
        
        self.data = all_data

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> dict:
        return self.data[idx]


class DCCircuitRLTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è GRPO –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ DC —Ü–µ–ø–µ–π"""

    def __init__(self, config: TrainingConfig = None, circuit_config: CircuitConfig = None, verifier_config: VerifierConfig = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–µ—Ä —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        self.config = config or CONFIG
        self.circuit_config = circuit_config or CircuitConfig()
        self.verifier_config = verifier_config or VerifierConfig()
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self._verifier = None
        self.dataset = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º dataset –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–∏
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è LLM
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.llm_log_file = f"llm_logs_{timestamp}.jsonl"
        self.log_entries = []

    def setup_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å LoRA"""        
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.model_name,
            max_seq_length=self.config.max_seq_length,
            load_in_4bit=self.config.use_4bit,  
            dtype=self.config.dtype,
            fast_inference=True,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            use_flash_attention=self.config.use_flash_attention,
            device_map="auto"  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU
        )
        
        if self.tokenizer.chat_template is None:
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π chat template –¥–ª—è Qwen3
            self.tokenizer.chat_template = """{% for message in messages %}
{% if message['role'] == 'system' %}
<|im_start|>system
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'user' %}
<|im_start|>user
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'assistant' %}
<|im_start|>assistant
{{ message['content'] }}<|im_end|>
{% endif %}
{% endfor %}"""
        
        # LoRA –æ–±—É—á–µ–Ω–∏–µ
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.config.lora_r,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=3407,
        )
        
        self.model.train()
        self.model.print_trainable_parameters()

    def log_llm_interaction(self, prompt, completion, reward=None, metadata=None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å LLM"""
        entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "prompt": prompt,
            "completion": completion,
            "reward": reward,
            "metadata": metadata or {}
        }
        self.log_entries.append(entry)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
        with open(self.llm_log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')

    def save_llm_logs(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –ª–æ–≥–∏ LLM"""
        with open(f"llm_logs_complete_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
            json.dump(self.log_entries, f, ensure_ascii=False, indent=2)
    
    def reward_function(self, prompts, completions, **kwargs) -> List[float]:
        """Reward –Ω–∞ –æ—Å–Ω–æ–≤–µ verifier.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ dataset.
        """
        if self._verifier is None:
            self._verifier = DCCircuitVerifier(self.verifier_config)
        
        if self.dataset is None:
            raise ValueError("dataset not initialized")
        
        # –í—ã—á–∏—Å–ª—è–µ–º rewards –¥–ª—è –∫–∞–∂–¥–æ–≥–æ completion
        rewards = []
        for idx, completion in enumerate(completions):
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –≤ dataset –ø–æ –ø—Ä–æ–º–ø—Ç—É
            prompt_text = prompts[idx] if isinstance(prompts[idx], str) else str(prompts[idx])
            
            # –ò—â–µ–º –≤ dataset –ø–æ –≤–æ–ø—Ä–æ—Å—É –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
            correct_answer = None
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É (–µ—Å–ª–∏ –ø—Ä–æ–º–ø—Ç—ã –∏–¥—É—Ç –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ)
            if idx < len(self.dataset):
                correct_answer = self.dataset[idx]["answer"]
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç –ø–æ –∏–Ω–¥–µ–∫—Å—É {idx}: {correct_answer}")
            else:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                for data_item in self.dataset:
                    if data_item["question"] in prompt_text:
                        correct_answer = data_item["answer"]
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É: {correct_answer}")
                        break
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            accuracy_score = None
            reward = 0.0
            extracted_answer = None
            
            if correct_answer is None:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π reward
                print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {idx}")
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞
                completion_str = str(completion)
                has_correct_format = "<think>" in completion_str and "<answer>" in completion_str
                has_tool_call = "<tool_call>" in completion_str
                
                if has_tool_call and not has_correct_format:
                    print(f"‚ö†Ô∏è  –ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç tool_call –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {idx} - —à—Ç—Ä–∞—Ñ!")
                    # –®—Ç—Ä–∞—Ñ—É–µ–º –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω–æ–≥–æ tool_call —Ñ–æ—Ä–º–∞—Ç–∞
                    reward = -1.0  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π reward –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                    accuracy_score = 0.0
                    print(f"‚ùå –®—Ç—Ä–∞—Ñ –∑–∞ tool_call: reward = {reward}")
                    print(f"üîç Debug: completion type = {type(completion)}")
                    print(f"üîç Debug: completion_str[:200] = {completion_str[:200]}")
                else:
                    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π Data –æ–±—ä–µ–∫—Ç –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
                    data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º completion –≤ —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                    completion_str_for_verifier = str(completion) if not isinstance(completion, str) else completion
                    accuracy_score = self._verifier.get_accuracy_score(data, completion_str_for_verifier)
                    reward = accuracy_score * 2.0  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º reward [0, 2]
                    print(f"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, accuracy: {accuracy_score:.3f}, reward: {reward:.3f}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å LLM
            self.log_llm_interaction(
                prompt=prompt_text,
                completion=completion,
                reward=reward,
                metadata={
                    "correct_answer": correct_answer,
                    "accuracy_score": accuracy_score if correct_answer else None,
                    "batch_idx": idx,
                    "completion_has_tool_call": "<tool_call>" in str(completion),
                    "completion_has_think": "<think>" in str(completion),
                    "completion_has_answer": "<answer>" in str(completion),
                    "parsed_from_tool_call": has_tool_call and not has_correct_format,
                    "extracted_answer": extracted_answer if 'extracted_answer' in locals() else None
                }
            )
            
            rewards.append(reward)

        return rewards

    def setup_trainer(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ GRPO —Ç—Ä–µ–Ω–µ—Ä–∞."""
        train_dataset = DCCircuitDataset(self.config, self.circuit_config, self.verifier_config)
        self.dataset = train_dataset  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–∏
        
        training_args = GRPOConfig(
            use_vllm=True, 
            learning_rate=self.config.learning_rate,
            adam_beta1=self.config.adam_beta1,
            adam_beta2=self.config.adam_beta2,
            weight_decay=self.config.weight_decay,
            warmup_ratio=self.config.warmup_ratio,
            lr_scheduler_type="cosine",
            optim="adamw_8bit",
            logging_steps=1,
            per_device_train_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations,
            max_prompt_length=4096, 
            max_completion_length=self.config.max_completion_length,  
            max_steps=self.config.max_steps,
            save_steps=self.config.save_steps,
            max_grad_norm=self.config.max_grad_norm,
            report_to="none",
            output_dir=self.config.output_dir,
            temperature=self.config.temperature,
            repetition_penalty=self.config.repetition_penalty,
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ GRPO
        self.model.train()
        self.trainer = GRPOTrainer(
            model=self.model,
            processing_class=self.tokenizer,
            reward_funcs=[self.reward_function],
            args=training_args,
            train_dataset=train_dataset,
        )
        

    def train(self):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è."""
        try:        
            self.model.train()
            self.trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ GRPO
            self.trainer.save_model(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ LLM
            self.save_llm_logs()
            print(f"‚úÖ –õ–æ–≥–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.llm_log_file}")
            
        except KeyboardInterrupt:
            checkpoint_dir = f"{self.config.output_dir}/checkpoint"
            os.makedirs(checkpoint_dir, exist_ok=True)
            self.trainer.save_model(checkpoint_dir)
            self.tokenizer.save_pretrained(checkpoint_dir)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –ø—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏
            self.save_llm_logs()
            print(f"‚úÖ –õ–æ–≥–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.llm_log_file}")
        except Exception as e:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.save_llm_logs()
            print(f"‚úÖ –õ–æ–≥–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.llm_log_file}")
            raise

    def run(self):
        """–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è GRPO."""
        self.setup_model()
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        self.setup_trainer()
        print("‚úÖ –¢—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        self.train()
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

if __name__ == "__main__":    
    trainer = DCCircuitRLTrainer(CONFIG)
    trainer.run()