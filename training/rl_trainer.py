"""
–ü–û–õ–ù–ê–Ø –°–ò–°–¢–ï–ú–ê RL –û–ë–£–ß–ï–ù–ò–Ø –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –≠–õ–ï–ö–¢–†–ò–ß–ï–°–ö–ò–• –¶–ï–ü–ï–ô

–†–µ–∞–ª–∏–∑—É–µ—Ç GRPO (Generative Reward Policy Optimization) –æ–±—É—á–µ–Ω–∏–µ
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º unsloth, TRL –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö reward functions.
"""

import os
import sys
import json
import argparse
import re
from typing import Dict, List
from dataclasses import dataclass

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from unsloth import FastLanguageModel
from trl import GRPOTrainer, GRPOConfig
from accelerate import Accelerator

from training.datasets import DCCircuitIterableDataset
from dc_circuit.game import DCCircuitGame


@dataclass
class TrainingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    model_name: str = "unsloth/Qwen2.5-3B-Instruct"
    output_dir: str = "./dc_circuit_model_rl"
    max_seq_length: int = 1024
    load_in_4bit: bool = True
    fast_inference: bool = True
    gpu_memory_utilization: float = 0.9

    # GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    num_train_epochs: int = 1
    per_device_train_batch_size: int = 1
    gradient_accumulation_steps: int = 1
    num_generations: int = 8
    learning_rate: float = 5e-6
    max_steps: int = 250
    logging_steps: int = 1
    save_steps: int = 250
    max_prompt_length: int = 256
    max_completion_length: int = 200
    temperature: float = 0.7
    beta: float = 0.04

    # Multi-GPU –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    num_gpus: int = 2  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU (1 –∏–ª–∏ 2 –¥–ª—è T4)
    gpu_memory_utilization: float = 0.9

    # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
    lora_r: int = 64
    lora_alpha: int = 64
    lora_dropout: float = 0.05
    lora_target_modules: List[str] = None
    use_gradient_checkpointing: str = "unsloth"

    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    adam_beta1: float = 0.9
    adam_beta2: float = 0.99
    weight_decay: float = 0.1
    warmup_ratio: float = 0.1
    max_grad_norm: float = 0.1
    optim: str = "adamw_8bit"

    def __post_init__(self):
        if self.lora_target_modules is None:
            self.lora_target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]


class DCCircuitRLTrainer:
    """
    –ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ RL –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GRPO –∞–ª–≥–æ—Ä–∏—Ç–º —Å reward function –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤
    """

    def __init__(self, config: TrainingConfig):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RL —Ç—Ä–µ–Ω–µ—Ä–∞

        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        self.config = config
        self.accelerator = Accelerator()

        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RL —Ç—Ä–µ–Ω–µ—Ä–∞...")
        print(f"üìã –ú–æ–¥–µ–ª—å: {config.model_name}")
        print(f"üéØ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {config.output_dir}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.game = DCCircuitGame()
        self.tokenizer = None
        self.model = None
        self.trainer = None

    def setup_model_and_tokenizer(self):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä

        üîß –®–ê–ì–ò:
        1. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (unsloth –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ)
        2. –î–æ–±–∞–≤–ª—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
        3. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")

        if UNSLOTH_AVAILABLE:
            print("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º unsloth –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏...")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å unsloth
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=self.config.model_name,
                max_seq_length=self.config.max_seq_length,
                load_in_4bit=self.config.load_in_4bit,
                fast_inference=self.config.fast_inference,
                max_lora_rank=self.config.lora_r,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
            )

            # –î–æ–±–∞–≤–ª—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
            self.model = FastLanguageModel.get_peft_model(
                self.model,
                r=self.config.lora_r,
                target_modules=self.config.lora_target_modules,
                lora_alpha=self.config.lora_alpha,
                use_gradient_checkpointing=self.config.use_gradient_checkpointing,
                random_state=3407,
            )

        else:
            print("‚ö†Ô∏è unsloth –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥...")
            print("üí° –î–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install unsloth")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name,
                trust_remote_code=True
            )

            # –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å multi-GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
            if self.config.num_gpus > 1:
                print(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è {self.config.num_gpus} GPU...")

                # –î–ª—è multi-GPU –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map
                device_map = "auto" if self.config.num_gpus == 1 else "balanced"

                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.model_name,
                    torch_dtype=torch.bfloat16 if is_bfloat16_supported() else torch.float16,
                    device_map=device_map,
                    trust_remote_code=True,
                    attn_implementation="flash_attention_2" if torch.cuda.is_available() else None,
                    max_memory={i: f"{int(self.config.gpu_memory_utilization * 16)}GiB" for i in range(self.config.num_gpus)}
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.model_name,
                    torch_dtype=torch.bfloat16 if is_bfloat16_supported() else torch.float16,
                    device_map="auto",
                    trust_remote_code=True,
                    attn_implementation="flash_attention_2" if torch.cuda.is_available() else None
                )

            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
            lora_config = LoraConfig(
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=self.config.lora_target_modules,
                task_type="CAUSAL_LM"
            )

            self.model = get_peft_model(self.model, lora_config)

    def get_system_prompt(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏

        Returns:
            –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
        """
        return (
            "You are an expert circuit analysis engineer. Solve electrical circuit problems using physics laws.\n\n"
            "Respond in the following format:\n"
            "<think>Reason step by step briefly.</think>\n"
            "<answer>Return ONLY the final number with exactly 3 decimal places (e.g., 1.234), no units.</answer>"
        )

    def extract_answer(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ <answer> —Ç–µ–≥–æ–≤"""
        answer_start = text.find("<answer>")
        answer_end = text.find("</answer>")

        if answer_start != -1 and answer_end != -1:
            return text[answer_start + 8:answer_end].strip()
        return ""

    def correctness_reward_func(self, completions, prompts, answer, **kwargs) -> List[float]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è reward function - —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞

        Args:
            completions: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏
            prompts: –ü—Ä–æ–º–ø—Ç—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
            answer: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π (2.0 –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, 0.0 –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π)
        """
        responses = [completion[0]['content'] for completion in completions]
        extracted_responses = [self.extract_answer(r) for r in responses]

        print('-' * 20, f"Question:\n{prompts[0][-1]['content']}",
              f"\nAnswer:\n{answer[0]}",
              f"\nResponse:\n{responses[0]}",
              f"\nExtracted:\n{extracted_responses[0]}")

        return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]

    def numeric_reward_func(self, completions, **kwargs) -> List[float]:
        """Reward –∑–∞ —á–∏—Å–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        responses = [completion[0]['content'] for completion in completions]
        extracted_responses = [self.extract_answer(r) for r in responses]
        return [0.5 if r.replace('.', '').replace('-', '').isdigit() else 0.0 for r in extracted_responses]

    def strict_format_reward_func(self, completions, **kwargs) -> List[float]:
        """Reward –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞"""
        pattern = r"^<reasoning>\n.*?\n</reasoning>\n<answer>\n.*?\n</answer>\n$"
        responses = [completion[0]["content"] for completion in completions]
        matches = [re.match(pattern, r, re.DOTALL) for r in responses]
        return [0.5 if match else 0.0 for match in matches]

    def soft_format_reward_func(self, completions, **kwargs) -> List[float]:
        """Reward –∑–∞ –º—è–≥–∫–∏–π —Ñ–æ—Ä–º–∞—Ç (—Ç–µ–≥–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –ª—é–±–æ–º –ø–æ—Ä—è–¥–∫–µ)"""
        pattern = r"<reasoning>.*?</reasoning>\s*<answer>.*?</answer>"
        responses = [completion[0]["content"] for completion in completions]
        matches = [re.search(pattern, r, re.DOTALL) for r in responses]
        return [0.5 if match else 0.0 for match in matches]

    def xml_count_reward_func(self, completions, **kwargs) -> List[float]:
        """Reward –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ XML —Ç–µ–≥–æ–≤"""
        contents = [completion[0]["content"] for completion in completions]

        def count_xml(text) -> float:
            count = 0.0
            if text.count("<reasoning>\n") == 1:
                count += 0.125
            if text.count("\n</reasoning>\n") == 1:
                count += 0.125
            if text.count("\n<answer>\n") == 1:
                count += 0.125
            if text.count("\n</answer>") == 1:
                count += 0.125
            return count

        return [count_xml(c) for c in contents]

    def get_reward_functions(self) -> List:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö reward functions"""
        return [
            self.xml_count_reward_func,
            self.soft_format_reward_func,
            self.strict_format_reward_func,
            self.numeric_reward_func,
            self.correctness_reward_func,
        ]

    def create_training_dataset(self):
        """
        –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Returns:
            –ò—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        print("üìö –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

        return DCCircuitIterableDataset(
            difficulties=[1, 3, 5, 7],  # –†–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            samples_per_difficulty=10  # –û–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π —É—Ä–æ–≤–µ–Ω—å
        )

    def format_prompt(self, example: Dict[str, str]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            example: –ü—Ä–∏–º–µ—Ä —Å –≤–æ–ø—Ä–æ—Å–æ–º –∏ –æ—Ç–≤–µ—Ç–æ–º

        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """
        question = example["question"]
        answer = example["answer"]

        system_prompt = self.get_system_prompt()

        return (
            f"{system_prompt}\n\n"
            f"Question: {question}\n"
            f"Answer: <gold>{answer}</gold>"
        )

    def setup_trainer(self):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç GRPO —Ç—Ä–µ–Ω–µ—Ä

        üîß –®–ê–ì–ò:
        1. –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç
        2. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç training arguments
        3. –°–æ–∑–¥–∞–µ—Ç GRPOTrainer —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ reward functions
        """
        print("üéØ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GRPO —Ç—Ä–µ–Ω–µ—Ä–∞...")

        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        train_dataset = self.create_training_dataset()

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º training arguments (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
        training_args = GRPOConfig(
            use_vllm=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º vLLM –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ inference
            learning_rate=self.config.learning_rate,
            adam_beta1=self.config.adam_beta1,
            adam_beta2=self.config.adam_beta2,
            weight_decay=self.config.weight_decay,
            warmup_ratio=self.config.warmup_ratio,
            lr_scheduler_type="cosine",
            optim=self.config.optim,
            logging_steps=self.config.logging_steps,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations,
            max_prompt_length=self.config.max_prompt_length,
            max_completion_length=self.config.max_completion_length,
            max_steps=self.config.max_steps,
            save_steps=self.config.save_steps,
            max_grad_norm=self.config.max_grad_norm,
            report_to="none",  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Weights & Biases
            output_dir=self.config.output_dir,
        )

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ reward functions
        reward_functions = self.get_reward_functions()

        # –°–æ–∑–¥–∞–µ–º GRPO —Ç—Ä–µ–Ω–µ—Ä
        self.trainer = GRPOTrainer(
            model=self.model,
            processing_class=self.tokenizer,
            reward_funcs=reward_functions,
            args=training_args,
            train_dataset=train_dataset,
        )


    def train(self):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ

        üîÑ –ü–†–û–¶–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø:
        1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö
        2. –¶–∏–∫–ª GRPO: generate ‚Üí reward ‚Üí optimize
        3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        4. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            self.trainer.train()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            self.trainer.save_model(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)

            print("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.config.output_dir}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.save_training_stats()

        except KeyboardInterrupt:
            print("\n‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            self.save_checkpoint()
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            self.save_checkpoint()

    def save_training_stats(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            stats = {
                "config": {
                    "model_name": self.config.model_name,
                    "max_steps": self.config.max_steps,
                    "learning_rate": self.config.learning_rate,
                    "lora_r": self.config.lora_r,
                    "lora_alpha": self.config.lora_alpha,
                },
                "training_completed": True,
                "output_dir": self.config.output_dir
            }

            os.makedirs(self.config.output_dir, exist_ok=True)
            with open(f"{self.config.output_dir}/training_stats.json", "w", encoding="utf-8") as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)

            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.config.output_dir}/training_stats.json")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

    def save_checkpoint(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç checkpoint –ø—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏
        """
        try:
            checkpoint_dir = f"{self.config.output_dir}/checkpoint"
            os.makedirs(checkpoint_dir, exist_ok=True)

            self.trainer.save_model(checkpoint_dir)
            self.tokenizer.save_pretrained(checkpoint_dir)

            print(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {checkpoint_dir}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint: {e}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ GRPO –æ–±—É—á–µ–Ω–∏—è —Å unsloth
    """
    parser = argparse.ArgumentParser()

    # –ú–æ–¥–µ–ª—å –∏ –±–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    parser.add_argument("--model", default="unsloth/Qwen2.5-3B-Instruct",
                       help="–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è unsloth)")
    parser.add_argument("--output-dir", default="./dc_circuit_model_rl",
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    parser.add_argument("--max-seq-length", type=int, default=1024,
                       help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

    # –û–±—É—á–µ–Ω–∏–µ
    parser.add_argument("--max-steps", type=int, default=250,
                       help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--learning-rate", type=float, default=5e-6,
                       help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--batch-size", type=int, default=1,
                       help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--gradient-accumulation", type=int, default=1,
                       help="Gradient accumulation steps")
    parser.add_argument("--num-generations", type=int, default=8,
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –Ω–∞ —à–∞–≥")

    # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--lora-r", type=int, default=64,
                       help="LoRA rank (—á–µ–º –±–æ–ª—å—à–µ - —Ç–µ–º —É–º–Ω–µ–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
    parser.add_argument("--lora-alpha", type=int, default=64,
                       help="LoRA alpha")
    parser.add_argument("--lora-dropout", type=float, default=0.05,
                       help="LoRA dropout")

    # vLLM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--load-in-4bit", action="store_true", default=True,
                       help="–ó–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å –≤ 4-bit –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")
    parser.add_argument("--fast-inference", action="store_true", default=True,
                       help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å vLLM –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ inference")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.9,
                       help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏")
    parser.add_argument("--num-gpus", type=int, default=1,
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU (1 –∏–ª–∏ 2 –¥–ª—è T4)")

    # GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="Sampling temperature")
    parser.add_argument("--beta", type=float, default=0.04,
                       help="KL divergence coefficient")

    args = parser.parse_args()


    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = TrainingConfig(
        model_name=args.model,
        output_dir=args.output_dir,
        max_seq_length=args.max_seq_length,
        load_in_4bit=args.load_in_4bit,
        fast_inference=args.fast_inference,
        gpu_memory_utilization=args.gpu_memory_utilization,
        num_gpus=args.num_gpus,
        max_steps=args.max_steps,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        num_generations=args.num_generations,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        temperature=args.temperature,
        beta=args.beta
    )

    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = DCCircuitRLTrainer(config)

    try:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        trainer.setup_model_and_tokenizer()

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        trainer.setup_trainer()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer.train()

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
