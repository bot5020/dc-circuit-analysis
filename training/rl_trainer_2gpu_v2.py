"""GRPO –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è 2x GPU - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –ë–ï–ó DDP"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dataclasses import dataclass
from typing import List

import torch
from torch.utils.data import Dataset
from unsloth import FastLanguageModel
from trl import GRPOTrainer, GRPOConfig

from base.utils import extract_answer
from base.data import Data
from dc_circuit.game import DCCircuitGame
from dc_circuit.verifier import DCCircuitVerifier


print("="*80)
print("üîç –ü–†–û–í–ï–†–ö–ê GPU")
print("="*80)
print(f"GPU count: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"GPU {i}: {torch.cuda.get_device_properties(i).name}")
print("="*80)


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

@dataclass
class TrainingConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è 2 GPU —á–µ—Ä–µ–∑ DataParallel (–ü–†–û–©–ï –ß–ï–ú DDP)"""
    
    # –ú–æ–¥–µ–ª—å
    model_name: str = "Qwen/Qwen3-4B-Instruct-2507"
    output_dir: str = "./dc_circuit_model_rl_2gpu_v2"
    max_seq_length: int = 2048
    
    # LoRA
    lora_r: int = 64
    lora_alpha: int = 64
    lora_dropout: float = 0.0  # –°—Ç–∞–≤–∏–º 0 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    
    # –û–±—É—á–µ–Ω–∏–µ (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è 2 GPU)
    learning_rate: float = 1e-5
    max_steps: int = 500
    batch_size: int = 8  # –ù–∞ –∫–∞–∂–¥–æ–π GPU
    gradient_accumulation_steps: int = 2
    num_generations: int = 2
    save_steps: int = 50
    
    # Dataset
    difficulties: List[int] = None
    samples_per_difficulty: int = 100
    
    def __post_init__(self):
        if self.difficulties is None:
            self.difficulties = [1, 2, 3, 4, 5]


CONFIG = TrainingConfig()


# ============================================================================
# DATASET (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ============================================================================

class DCCircuitDataset(Dataset):
    def __init__(self, config: TrainingConfig):
        self.game = DCCircuitGame()
        self.config = config
        self._data_cache = None

    def _generate_data(self) -> List[dict]:
        if self._data_cache is None:
            all_data = []
            for difficulty in self.config.difficulties:
                print(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ {difficulty}...")
                data_list = self.game.generate(
                    num_of_questions=self.config.samples_per_difficulty,
                    difficulty=difficulty,
                    max_attempts=30
                )
                print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data_list)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
                all_data.extend([{
                    "prompt": [
                        {"role": "user", "content": f"{data.question}\n<gold>{float(data.answer):.3f}</gold>"}
                    ],
                    "question": data.question,
                    "answer": f"{float(data.answer):.3f}",
                    "difficulty": data.difficulty
                } for data in data_list])
            
            self._data_cache = all_data
            print(f"üìä –í—Å–µ–≥–æ: {len(all_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        return self._data_cache

    def __len__(self) -> int:
        return len(self._generate_data())

    def __getitem__(self, idx: int) -> dict:
        return self._generate_data()[idx]


# ============================================================================
# TRAINER
# ============================================================================

class DCCircuitRLTrainer:
    def __init__(self, config: TrainingConfig = None):
        self.config = config or CONFIG
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self._verifier = None
        
        print(f"\nüéÆ GRPO Trainer –¥–ª—è 2x GPU (DataParallel)")
        print(f"Batch size: {self.config.batch_size} per GPU")
        print(f"GPU count: {torch.cuda.device_count()}")

    def setup_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –ë–ï–ó DDP - –∏—Å–ø–æ–ª—å–∑—É–µ–º DataParallel"""
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ {self.config.model_name}...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU 0
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.model_name,
            max_seq_length=self.config.max_seq_length,
            load_in_4bit=True,
            fast_inference=False,
            device_map="cuda:0"  # –Ø–≤–Ω–æ –Ω–∞ GPU 0
        )
        
        if self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ message['content'] }}{% endif %}{% endfor %}"
        
        # LoRA
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.config.lora_r,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=3407,
        )
        
        # –í–ê–†–ò–ê–ù–¢ 1: –ï—Å–ª–∏ –µ—Å—Ç—å 2+ GPU - –∏—Å–ø–æ–ª—å–∑—É–µ–º DataParallel
        if torch.cuda.device_count() > 1:
            print(f"üîó –ò—Å–ø–æ–ª—å–∑—É–µ–º {torch.cuda.device_count()} GPU —á–µ—Ä–µ–∑ DataParallel")
            self.model = torch.nn.DataParallel(self.model)
        
        self.model.train()
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # Print trainable parameters
        if hasattr(self.model, 'module'):
            self.model.module.print_trainable_parameters()
        else:
            self.model.print_trainable_parameters()

    def _extract_prompt_content(self, prompts) -> str:
        if not prompts:
            return ""
        if isinstance(prompts[0], str):
            return prompts[0]
        return prompts[0][-1]['content'] if prompts[0] else ""
    
    def _normalize_completions(self, completions) -> List[str]:
        responses = []
        for item in completions:
            if isinstance(item, str):
                responses.append(item)
            elif isinstance(item, dict):
                responses.append(item.get("content", ""))
            elif isinstance(item, list) and item:
                candidate = item[0]
                responses.append(candidate.get("content", "") if isinstance(candidate, dict) else str(candidate))
            else:
                responses.append(str(item))
        return responses
    
    def _should_log_step(self, step: int) -> bool:
        return (step in [1, 2, 3, 5, 10]) or (step > 10 and step % 20 == 0)
    
    def _extract_gold_answer(self, prompt_content: str) -> str:
        gold_start = prompt_content.find("<gold>")
        gold_end = prompt_content.find("</gold>")
        
        if gold_start != -1 and gold_end != -1:
            return prompt_content[gold_start + 6:gold_end].strip()
        return ""
    
    def _log_detailed_metrics(self, step: int, correct_answer: str, 
                             raw_response: str, extracted: str):
        print("\n" + "="*80)
        print(f"üìä Step {step}")
        print("="*80)
        print(f"\n‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π: {correct_answer}")
        print(f"\nü§ñ –ú–æ–¥–µ–ª—å: {raw_response[:150]}...")
        print(f"\nüîç –ò–∑–≤–ª–µ—á—ë–Ω: '{extracted}'")
        
        if extracted:
            data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
            try:
                score = self._verifier.get_accuracy_score(data, raw_response)
                reward = score * 2.0
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç
                try:
                    model_val = float(extracted)
                    correct_val = float(correct_answer)
                    if abs(correct_val) > 1e-12:
                        rel_error = abs(model_val - correct_val) / abs(correct_val)
                        rel_error_percent = rel_error * 100
                        print(f"\nüî¨ –ü–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å: {rel_error_percent:.2f}%")
                except:
                    pass
                
                print(f"\nüí∞ Score: {score:.2f} ‚Üí Reward: {reward:.2f}")
            except Exception as e:
                print(f"\nüí∞ Reward: 0.0 ({e})")
        
        print("="*80 + "\n")
    
    def _calculate_rewards(self, correct_answer: str, responses: List[str]) -> List[float]:
        data = Data(question="", answer=correct_answer, difficulty=1, metadata={})
        rewards = []
        
        for response in responses:
            try:
                accuracy_score = self._verifier.get_accuracy_score(data, response)
                reward = accuracy_score * 2.0
            except Exception:
                reward = 0.0
            rewards.append(reward)
        
        return rewards
    
    def reward_function(self, prompts, completions, **kwargs) -> List[float]:
        if self._verifier is None:
            self._verifier = DCCircuitVerifier()
        
        responses = self._normalize_completions(completions)
        
        trainer_state = kwargs.get('trainer_state')
        step = getattr(trainer_state, 'global_step', 0) if trainer_state else 0
        
        if self._should_log_step(step) and prompts and responses:
            prompt_content = self._extract_prompt_content(prompts)
            correct_answer = self._extract_gold_answer(prompt_content)
            
            if correct_answer:
                raw_response = responses[0] if responses else ""
                extracted = extract_answer(raw_response) if responses else ""
                self._log_detailed_metrics(step, correct_answer, raw_response, extracted)
        
        prompt_content = self._extract_prompt_content(prompts)
        correct_answer = self._extract_gold_answer(prompt_content)
        
        if not correct_answer:
            return [0.0] * len(responses)
        
        return self._calculate_rewards(correct_answer, responses)

    def setup_trainer(self):
        train_dataset = DCCircuitDataset(self.config)
    
        training_args = GRPOConfig(
            use_vllm=True,
            learning_rate=self.config.learning_rate,
            adam_beta1=0.9,
            adam_beta2=0.99,
            weight_decay=0.1,
            warmup_ratio=0.1,
            lr_scheduler_type="cosine",
            optim="adamw_8bit",
            logging_steps=1,
            per_device_train_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations,
            max_prompt_length=4096,
            max_completion_length=2048,
            max_steps=self.config.max_steps,
            save_steps=self.config.save_steps,
            max_grad_norm=0.1,
            report_to="none",
            output_dir=self.config.output_dir,
            temperature=0.7,
            repetition_penalty=1.1,
        )
        
        # –í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º –º–æ–¥–µ–ª—å –° DataParallel –æ–±–µ—Ä—Ç–∫–æ–π!
        # GRPOTrainer –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å DataParallel —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–µ GPU
        self.trainer = GRPOTrainer(
            model=self.model,  # ‚Üê –ü–µ—Ä–µ–¥–∞—ë–º –∫–∞–∫ –µ—Å—Ç—å, —Å DataParallel!
            processing_class=self.tokenizer,
            reward_funcs=[self.reward_function],
            args=training_args,
            train_dataset=train_dataset,
        )
        
        print("‚úÖ GRPO —Ç—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

    def train(self):
        try:
            self.model.train()
            self.trainer.train()
            
            self.trainer.save_model(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.config.output_dir}")
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ")
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            raise

    def run(self):
        self.setup_model()
        self.setup_trainer()
        self.train()


if __name__ == "__main__":
    print("\n" + "="*80)
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø (DataParallel)")
    print("="*80)
    print(f"GPU –¥–æ—Å—Ç—É–ø–Ω–æ: {torch.cuda.device_count()}")
    print(f"Effective batch: {CONFIG.batch_size * CONFIG.gradient_accumulation_steps * torch.cuda.device_count()}")
    print("="*80 + "\n")
    
    trainer = DCCircuitRLTrainer(CONFIG)
    trainer.run()
    
    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
