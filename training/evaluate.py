"""–û—Ü–µ–Ω–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
from typing import Dict, List
import matplotlib.pyplot as plt
import numpy as np

from base.data import Data
from base.utils import get_system_prompt
from dc_circuit.game import DCCircuitGame


def evaluate_model(
    model_generate_func,
    test_data: List[Data],
    max_samples: int = 100
) -> Dict:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        model_generate_func: –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ model(question) -> answer
        test_data: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á
        max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: {accuracy, correct, total}
    """
    game = DCCircuitGame()
    correct_count = 0
    total_count = min(len(test_data), max_samples)

    
    for i, data_item in enumerate(test_data[:total_count]):
        if i % 10 == 0 and i > 0:
            print(f"   –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {i}/{total_count}...")
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏
            model_response = model_generate_func(data_item.question)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ verifier
            if game.verify(data_item, model_response):
                correct_count += 1
                
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –Ω–∞ –∑–∞–¥–∞—á–µ {i}: {e}")
            continue
    
    accuracy = correct_count / total_count if total_count > 0 else 0.0
    
    return {
        "accuracy": accuracy,
        "correct": correct_count,
        "total": total_count
    }


def plot_model_comparison(
    baseline_results: Dict[int, float],
    trained_results: Dict[int, float],
    save_path: str = "reports/model_comparison.png"
) -> None:
    """–°–æ–∑–¥–∞—ë—Ç –ø–∞—Ä–Ω—É—é –±–∞—Ä–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
    
    Args:
        baseline_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã baseline –º–æ–¥–µ–ª–∏ {difficulty: accuracy}
        trained_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ {difficulty: accuracy}
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
    """
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–∏—Ö —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    difficulties = sorted(set(baseline_results.keys()) & set(trained_results.keys()))
    
    if not difficulties:
        return
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    baseline_accuracies = [baseline_results[d] for d in difficulties]
    trained_accuracies = [trained_results[d] for d in difficulties]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å –ø–∞—Ä–Ω—ã–º–∏ –±–∞—Ä–∞–º–∏
    _, ax = plt.subplots(figsize=(14, 6))
    
    x = np.arange(len(difficulties))
    width = 0.35  # –®–∏—Ä–∏–Ω–∞ –±–∞—Ä–∞
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä–Ω—ã—Ö –±–∞—Ä–æ–≤
    bars1 = ax.bar(
        x - width/2, 
        baseline_accuracies, 
        width, 
        label='Baseline Model',
        color='skyblue',
        edgecolor='black'
    )
    bars2 = ax.bar(
        x + width/2, 
        trained_accuracies, 
        width, 
        label='Trained Model',
        color='lightcoral',
        edgecolor='black'
    )
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –±–∞—Ä–∞—Ö
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.annotate(
                f'{height:.2f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),
                textcoords="offset points",
                ha='center', 
                va='bottom', 
                fontsize=9
            )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    ax.set_xlabel('–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏', fontsize=12, fontweight='bold')
    ax.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å', fontsize=12, fontweight='bold')
    ax.set_title(
        '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: Baseline vs Trained\n(DC Circuit Analysis Tasks)', 
        fontsize=14, 
        fontweight='bold', 
        pad=20
    )
    ax.set_xticks(x)
    ax.set_xticklabels([f'–£—Ä–æ–≤–µ–Ω—å {d}' for d in difficulties])
    ax.legend(loc='upper right', fontsize=11, framealpha=0.9)
    ax.grid(True, axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim(0, max(max(baseline_accuracies), max(trained_accuracies)) * 1.15)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")


def generate_evaluation_report(
    baseline_results: Dict[int, float],
    trained_results: Dict[int, float],
    baseline_model: str = "baseline",
    trained_model: str = "trained",
    save_dir: str = "reports"
) -> None:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –æ—Ü–µ–Ω–∫–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ JSON.
    
    Args:
        baseline_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã baseline –º–æ–¥–µ–ª–∏
        trained_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        baseline_model: –ù–∞–∑–≤–∞–Ω–∏–µ baseline –º–æ–¥–µ–ª–∏
        trained_model: –ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤
    """
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π
    difficulties = sorted(set(baseline_results.keys()) & set(trained_results.keys()))
    improvements = {}
    
    for difficulty in difficulties:
        baseline_acc = baseline_results[difficulty]
        trained_acc = trained_results[difficulty]
        improvement = trained_acc - baseline_acc
        improvements[difficulty] = improvement
    
        print(f"–°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}: {baseline_acc:.3f} ‚Üí {trained_acc:.3f} ({improvement:+.3f})")
    
    # –°—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    if improvements:
        avg_improvement = sum(improvements.values()) / len(improvements)
        print(f"\n–°—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {avg_improvement:+.3f}")
        
        if avg_improvement > 0:
            print("–£–°–ü–ï–•: –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∏–ª–∞—Å—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è!")
    else:
        avg_improvement = 0.0
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    plot_model_comparison(
        baseline_results, 
        trained_results, 
        save_path=f"{save_dir}/model_comparison.png"
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON
    results_data = {
        "baseline_model": baseline_model,
        "trained_model": trained_model,
        "baseline_results": baseline_results,
        "trained_results": trained_results,
        "improvements": improvements,
        "avg_improvement": avg_improvement
    }
    
    os.makedirs(save_dir, exist_ok=True)
    json_path = f"{save_dir}/evaluation_results.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    
    print(f"JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {json_path}")


def load_model(model_path: str, max_seq_length: int = 3072):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.
    
    Args:
        model_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –º–æ–¥–µ–ª—å—é
        max_seq_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    Returns:
        (model, tokenizer) - –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    """
    print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {model_path}")
    
    from unsloth import FastLanguageModel
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=max_seq_length,
        load_in_4bit=False,
        dtype=None,
    )
    
    # –í–∫–ª—é—á–∞–µ–º inference mode
    FastLanguageModel.for_inference(model)
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return model, tokenizer


def create_model_generator(model, tokenizer, max_new_tokens: int = 512):
    """–°–æ–∑–¥–∞—ë—Ç —Ñ—É–Ω–∫—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏.
    
    Args:
        model: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    
    Returns:
        –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: question -> answer
    """
    def generate(question: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å."""
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        messages = [
            {"role": "system", "content": get_system_prompt()},
            {"role": "user", "content": question}
        ]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.1,  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        return response
    
    return generate


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏."""
    import argparse
    
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ DC Circuit Analysis")
    parser.add_argument("--model_path", type=str, default="./dc_circuit_model_rl", help="–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--baseline_model", type=str, default="Qwen/Qwen3-4B-Instruct-2507", help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    parser.add_argument("--samples_per_difficulty", type=int, default=20, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –Ω–∞ —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏")
    parser.add_argument("--difficulties", type=str, default="1,2,3,4,5", help="–£—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é")
    parser.add_argument("--output_dir", type=str, default="./evaluation_reports", help="–ü–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤")
    
    args = parser.parse_args()
    
    difficulties = [int(d) for d in args.difficulties.split(",")]
    
    print("=" * 80)
    print("üî¨ –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò DC CIRCUIT ANALYSIS")
    print("=" * 80)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    game = DCCircuitGame()
    test_data_by_difficulty = {}
    
    for difficulty in difficulties:
        print(f"  –°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è {args.samples_per_difficulty} –∑–∞–¥–∞—á...")
        data = game.generate(
            num_of_questions=args.samples_per_difficulty,
            difficulty=difficulty,
            max_attempts=50
        )
        test_data_by_difficulty[difficulty] = data
        print(f"    ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data)} –∑–∞–¥–∞—á")
    
    total_tasks = sum(len(data) for data in test_data_by_difficulty.values())
    print(f"üìä –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á: {total_tasks}")
    
    # –û—Ü–µ–Ω–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    print(f"\nüéØ –û–¶–ï–ù–ö–ê –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò: {args.model_path}")
    print("-" * 80)
    
    if os.path.exists(args.model_path):
        trained_model, trained_tokenizer = load_model(args.model_path)
        trained_generator = create_model_generator(trained_model, trained_tokenizer)
        
        trained_results = {}
        for difficulty, data in test_data_by_difficulty.items():
            print(f"\n  üìù –°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}:")
            result = evaluate_model(trained_generator, data, max_samples=len(data))
            trained_results[difficulty] = result["accuracy"]
            print(f"    ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {result['accuracy']:.3f} ({result['correct']}/{result['total']})")
        
        overall_accuracy = sum(trained_results.values()) / len(trained_results)
        print(f"\n  üéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {overall_accuracy:.3f}")
    else:
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.model_path}")
        trained_results = {d: 0.0 for d in difficulties}
    
    # –û—Ü–µ–Ω–∫–∞ baseline –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    print(f"\nüìä –û–¶–ï–ù–ö–ê BASELINE –ú–û–î–ï–õ–ò: {args.baseline_model}")
    print("-" * 80)
    print("(–ó–∞–≥—Ä—É–∑–∫–∞ baseline –º–æ–¥–µ–ª–∏...)")
    
    try:
        baseline_model, baseline_tokenizer = load_model(args.baseline_model)
        baseline_generator = create_model_generator(baseline_model, baseline_tokenizer)
        
        baseline_results = {}
        for difficulty, data in test_data_by_difficulty.items():
            print(f"\n  üìù –°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}:")
            result = evaluate_model(baseline_generator, data, max_samples=len(data))
            baseline_results[difficulty] = result["accuracy"]
            print(f"    ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {result['accuracy']:.3f} ({result['correct']}/{result['total']})")
        
        overall_baseline = sum(baseline_results.values()) / len(baseline_results)
        print(f"\n  üéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {overall_baseline:.3f}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ baseline: {e}")
        print("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º baseline –æ—Ü–µ–Ω–∫—É...")
        baseline_results = {d: 0.0 for d in difficulties}
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    print("\n" + "=" * 80)
    print("üìà –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("=" * 80)
    
    generate_evaluation_report(
        baseline_results=baseline_results,
        trained_results=trained_results,
        baseline_model=args.baseline_model,
        trained_model=args.model_path,
        save_dir=args.output_dir
    )
    
    print("\n" + "=" * 80)
    print("‚úÖ –û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 80)
    print(f"üìÅ –û—Ç—á—ë—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output_dir}")


if __name__ == "__main__":
    main()

