"""–û—Ü–µ–Ω–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

–°–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –†–∞—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
- –°–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä–Ω—ã—Ö –±–∞—Ä–Ω—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
from typing import Dict, List
import matplotlib.pyplot as plt
import numpy as np

from base.data import Data
from dc_circuit.game import DCCircuitGame


def evaluate_model(
    model_generate_func,
    test_data: List[Data],
    max_samples: int = 100
) -> Dict:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        model_generate_func: –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ model(question) -> answer
        test_data: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á
        max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: {accuracy, correct, total}
    """
    game = DCCircuitGame()
    correct_count = 0
    total_count = min(len(test_data), max_samples)
    
    print(f"üìä –û—Ü–µ–Ω–∏–≤–∞–µ–º {total_count} –∑–∞–¥–∞—á...")
    
    for i, data_item in enumerate(test_data[:total_count]):
        if i % 10 == 0 and i > 0:
            print(f"   –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {i}/{total_count}...")
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
            model_response = model_generate_func(data_item.question)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ verifier
            if game.verify(data_item, model_response):
                correct_count += 1
                
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –Ω–∞ –∑–∞–¥–∞—á–µ {i}: {e}")
            continue
    
    accuracy = correct_count / total_count if total_count > 0 else 0.0
    
    return {
        "accuracy": accuracy,
        "correct": correct_count,
        "total": total_count
    }


def plot_model_comparison(
    baseline_results: Dict[int, float],
    trained_results: Dict[int, float],
    save_path: str = "reports/model_comparison.png"
) -> None:
    """–°–æ–∑–¥–∞—ë—Ç –ø–∞—Ä–Ω—É—é –±–∞—Ä–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
    
    Args:
        baseline_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã baseline –º–æ–¥–µ–ª–∏ {difficulty: accuracy}
        trained_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ {difficulty: accuracy}
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
    """
    # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–∏–µ —É—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    difficulties = sorted(set(baseline_results.keys()) & set(trained_results.keys()))
    
    if not difficulties:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        return
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    baseline_accuracies = [baseline_results[d] for d in difficulties]
    trained_accuracies = [trained_results[d] for d in difficulties]
    
    # –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ–∏–∫ —Å –ø–∞—Ä–Ω—ã–º–∏ –±–∞—Ä–∞–º–∏
    fig, ax = plt.subplots(figsize=(14, 6))
    
    x = np.arange(len(difficulties))
    width = 0.35  # –®–∏—Ä–∏–Ω–∞ –±–∞—Ä–æ–≤
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞—Ä–Ω—ã–µ –±–∞—Ä—ã
    bars1 = ax.bar(
        x - width/2, 
        baseline_accuracies, 
        width, 
        label='Baseline Model',
        color='skyblue',
        edgecolor='black'
    )
    bars2 = ax.bar(
        x + width/2, 
        trained_accuracies, 
        width, 
        label='Trained Model',
        color='lightcoral',
        edgecolor='black'
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –±–∞—Ä–∞—Ö
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.annotate(
                f'{height:.2f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),
                textcoords="offset points",
                ha='center', 
                va='bottom', 
                fontsize=9
            )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    ax.set_xlabel('Difficulty Level', fontsize=12, fontweight='bold')
    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    ax.set_title(
        'Model Performance Comparison: Baseline vs Trained\n(DC Circuit Analysis Tasks)', 
        fontsize=14, 
        fontweight='bold', 
        pad=20
    )
    ax.set_xticks(x)
    ax.set_xticklabels([f'Level {d}' for d in difficulties])
    ax.legend(loc='upper right', fontsize=11, framealpha=0.9)
    ax.grid(True, axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim(0, max(max(baseline_accuracies), max(trained_accuracies)) * 1.15)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")


def generate_evaluation_report(
    baseline_results: Dict[int, float],
    trained_results: Dict[int, float],
    baseline_model: str = "baseline",
    trained_model: str = "trained",
    save_dir: str = "reports"
) -> None:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –æ—Ü–µ–Ω–∫–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ JSON.
    
    Args:
        baseline_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã baseline –º–æ–¥–µ–ª–∏
        trained_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        baseline_model: –ù–∞–∑–≤–∞–Ω–∏–µ baseline –º–æ–¥–µ–ª–∏
        trained_model: –ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤
    """
    print("üìà –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç –æ—Ü–µ–Ω–∫–∏...")
    
    # –í—ã—á–∏—Å–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏—è
    difficulties = sorted(set(baseline_results.keys()) & set(trained_results.keys()))
    improvements = {}
    
    for difficulty in difficulties:
        baseline_acc = baseline_results[difficulty]
        trained_acc = trained_results[difficulty]
        improvement = trained_acc - baseline_acc
        improvements[difficulty] = improvement
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        status = "üìà" if improvement > 0 else "üìâ" if improvement < 0 else "‚û°Ô∏è"
        print(f"{status} –°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}: {baseline_acc:.3f} ‚Üí {trained_acc:.3f} ({improvement:+.3f})")
    
    # –°—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    if improvements:
        avg_improvement = sum(improvements.values()) / len(improvements)
        print(f"\nüéØ –°—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {avg_improvement:+.3f}")
        
        if avg_improvement > 0:
            print("üéâ –£–°–ü–ï–•: –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∏–ª–∞—Å—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è!")
        else:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ —É–ª—É—á—à–∏–ª–∞—Å—å. –ù—É–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ.")
    else:
        avg_improvement = 0.0
    
    # –°–æ–∑–¥–∞—ë–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    plot_model_comparison(
        baseline_results, 
        trained_results, 
        save_path=f"{save_dir}/model_comparison.png"
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
    results_data = {
        "baseline_model": baseline_model,
        "trained_model": trained_model,
        "baseline_results": baseline_results,
        "trained_results": trained_results,
        "improvements": improvements,
        "avg_improvement": avg_improvement
    }
    
    os.makedirs(save_dir, exist_ok=True)
    json_path = f"{save_dir}/evaluation_results.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}")
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_dir}/model_comparison.png")