"""C–∫—Ä–∏–ø—Ç –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è DC Circuit Analysis.

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞:
1. Zero-shot - –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
2. Prompt Engineering - –º–æ–¥–µ–ª—å —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
3. GRPO Trained - –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å LoRA
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import List, Dict
import torch
from unsloth import FastLanguageModel

from base.data import Data
from dc_circuit.game import DCCircuitGame
from config import CircuitConfig, VerifierConfig, TrainingConfig
from base.utils import get_system_prompt


class Evaluator:
    """–û—Ü–µ–Ω—â–∏–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π."""
    
    def __init__(
        self,
        baseline_model: str = "unsloth/qwen3-4b-instruct-2507-unsloth-bnb-4bit",
        trained_model_path: str = "./dc_circuit_model_rl",
        samples_per_difficulty: int = 20
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–∞.
        
        Args:
            baseline_model: –ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            trained_model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            samples_per_difficulty: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –Ω–∞ —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        """
        self.baseline_model_name = baseline_model
        self.trained_model_path = trained_model_path
        self.samples_per_difficulty = samples_per_difficulty
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.circuit_config = CircuitConfig()
        self.verifier_config = VerifierConfig()
        self.training_config = TrainingConfig()
        
        # Game –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.game = DCCircuitGame(self.circuit_config, self.verifier_config)
        
    def generate_test_data(self) -> Dict[int, List[Data]]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {difficulty: list_of_data}
        """
        print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        test_data = {}
        
        for difficulty in [1, 2, 3]:
            print(f"  –°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è {self.samples_per_difficulty} –∑–∞–¥–∞—á...")
            data_list = self.game.generate(
                num_of_questions=self.samples_per_difficulty,
                difficulty=difficulty
            )
            test_data[difficulty] = data_list
            print(f"    ‚úì –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data_list)} –∑–∞–¥–∞—á")
        
        total = sum(len(data) for data in test_data.values())
        print(f"  –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á: {total}\n")
        
        return test_data
    
    def load_model(self, model_path: str, is_trained: bool = False):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å.
        
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            is_trained: True –µ—Å–ª–∏ —ç—Ç–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å LoRA
        
        Returns:
            (model, tokenizer)
        """
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_path}")
        
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=self.training_config.max_seq_length,
            load_in_4bit=True,
            dtype=None,
            fast_inference=True,
            gpu_memory_utilization=0.55
        )
        
        # –†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        FastLanguageModel.for_inference(model)
        
        print(f"  ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")
        return model, tokenizer
    
    def generate_answer(
        self, 
        model, 
        tokenizer, 
        question: str, 
        use_system_prompt: bool = True
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–æ–ø—Ä–æ—Å.
        
        Args:
            model: –ú–æ–¥–µ–ª—å
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            question: –í–æ–ø—Ä–æ—Å
            use_system_prompt: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        if use_system_prompt:
            messages = [
                {"role": "system", "content": get_system_prompt()},
                {"role": "user", "content": question}
            ]
        else:
            # Zero-shot: —Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å, –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": question}
            ]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
        
        outputs = model.generate(
            **inputs,
            max_new_tokens=self.training_config.max_completion_length,
            temperature=0.7,  
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        return response
    
    def evaluate_model_on_data(
        self,
        model,
        tokenizer,
        test_data: Dict[int, List[Data]],
        method_name: str,
        use_system_prompt: bool = True
    ) -> Dict[int, float]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            model: –ú–æ–¥–µ–ª—å
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            test_data: –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            method_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
            use_system_prompt: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {difficulty: accuracy}
        """
        print(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {method_name}")
        results = {}
        
        for difficulty, data_list in sorted(test_data.items()):
            correct = 0
            total = len(data_list)
            
            for i, data in enumerate(data_list):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                response = self.generate_answer(
                    model, tokenizer, data.question, use_system_prompt
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å
                if self.game.verify(data, response):
                    correct += 1
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                if (i + 1) % 5 == 0 or (i + 1) == total:
                    print(f"  –°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}: {i+1}/{total} –∑–∞–¥–∞—á...", end='\r')
            
            accuracy = correct / total if total > 0 else 0.0
            results[difficulty] = accuracy
            print(f"  –°–ª–æ–∂–Ω–æ—Å—Ç—å {difficulty}: {correct}/{total} = {accuracy:.1%}    ")
        
        # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
        avg_accuracy = sum(results.values()) / len(results) if results else 0.0
        print(f"  üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {avg_accuracy:.1%}\n")
        
        return results
    
    def run_evaluation(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É –≤—Å–µ—Ö —Ç—Ä–µ—Ö –º–µ—Ç–æ–¥–æ–≤."""
        print("================================================")
        print("                –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô DC CIRCUIT ANALYSIS")
        print("================================================")
        
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_data = self.generate_test_data()
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ baseline –º–æ–¥–µ–ª–∏
        baseline_model, baseline_tokenizer = self.load_model(
            self.baseline_model_name, 
            is_trained=False
        )
        
        # 3. Zero-shot –æ—Ü–µ–Ω–∫–∞ (–±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞)
        print("-"*70)
        zero_shot_results = self.evaluate_model_on_data(
            baseline_model,
            baseline_tokenizer,
            test_data,
            "Zero-shot (no system prompt)",
            use_system_prompt=False
        )
        
        # 4. Prompt Engineering –æ—Ü–µ–Ω–∫–∞ (—Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º)
        print("-"*70)
        prompt_eng_results = self.evaluate_model_on_data(
            baseline_model,
            baseline_tokenizer,
            test_data,
            "Prompt Engineering (with system prompt)",
            use_system_prompt=True
        )
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        del baseline_model, baseline_tokenizer
        torch.cuda.empty_cache()
        
        # 5. GRPO Trained –æ—Ü–µ–Ω–∫–∞ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        print("-"*70)
        grpo_results = {}
        if os.path.exists(self.trained_model_path):
            trained_model, trained_tokenizer = self.load_model(
                self.trained_model_path,
                is_trained=True
            )
            
            grpo_results = self.evaluate_model_on_data(
                trained_model,
                trained_tokenizer,
                test_data,
                "GRPO Trained (with LoRA)",
                use_system_prompt=True
            )
            
            del trained_model, trained_tokenizer
            torch.cuda.empty_cache()
        else:
            print(f"‚ö†Ô∏è  –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.trained_model_path}")
            print(f"   –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É GRPO Trained\n")
            grpo_results = {1: 0.0, 2: 0.0, 3: 0.0}
        
        # 6. –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.print_summary(zero_shot_results, prompt_eng_results, grpo_results)
        
        return {
            "zero_shot": zero_shot_results,
            "prompt_engineering": prompt_eng_results,
            "grpo_trained": grpo_results
        }
    
    def print_summary(
        self,
        zero_shot: Dict[int, float],
        prompt_eng: Dict[int, float],
        grpo: Dict[int, float]
    ):
        """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Args:
            zero_shot: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Zero-shot
            prompt_eng: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Prompt Engineering
            grpo: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã GRPO
        """
        print("="*70)
        print(" üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
        print("="*70)
        print()
        print("| –ú–µ—Ç–æ–¥                  | –°–ª–æ–∂–Ω–æ—Å—Ç—å 1 | –°–ª–æ–∂–Ω–æ—Å—Ç—å 2 | –°–ª–æ–∂–Ω–æ—Å—Ç—å 3 | –°—Ä–µ–¥–Ω–µ–µ |")
        print("|------------------------|-------------|-------------|-------------|---------|")
        
        # Zero-shot
        avg_zero = sum(zero_shot.values()) / len(zero_shot) if zero_shot else 0.0
        print(f"| Zero-shot              | {zero_shot.get(1, 0.0):>10.1%} | "
              f"{zero_shot.get(2, 0.0):>10.1%} | {zero_shot.get(3, 0.0):>10.1%} | "
              f"{avg_zero:>6.1%} |")
        
        # Prompt Engineering
        avg_pe = sum(prompt_eng.values()) / len(prompt_eng) if prompt_eng else 0.0
        print(f"| Prompt Engineering     | {prompt_eng.get(1, 0.0):>10.1%} | "
              f"{prompt_eng.get(2, 0.0):>10.1%} | {prompt_eng.get(3, 0.0):>10.1%} | "
              f"{avg_pe:>6.1%} |")
        
        # GRPO Trained
        avg_grpo = sum(grpo.values()) / len(grpo) if grpo else 0.0
        print(f"| GRPO Trained           | {grpo.get(1, 0.0):>10.1%} | "
              f"{grpo.get(2, 0.0):>10.1%} | {grpo.get(3, 0.0):>10.1%} | "
              f"{avg_grpo:>6.1%} |")
    


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    evaluator = Evaluator(
        baseline_model="unsloth/qwen3-4b-instruct-2507-unsloth-bnb-4bit",
        trained_model_path="./dc_circuit_model_rl",
        samples_per_difficulty=20 
    )
    
    results = evaluator.run_evaluation()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
    import json
    with open("evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2)
    print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ evaluation_results.json")


if __name__ == "__main__":
    main()