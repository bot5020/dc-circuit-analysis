# ü§ñ –ü–æ–ª–Ω–∞—è –°–∏—Å—Ç–µ–º–∞ RL –û–±—É—á–µ–Ω–∏—è –¥–ª—è –ê–Ω–∞–ª–∏–∑–∞ –≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –¶–µ–ø–µ–π

## üéØ –û–±–∑–æ—Ä

**`rl_trainer.py`** - —ç—Ç–æ –ø–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ reinforcement learning –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **GRPO (Generative Reward Policy Optimization)** –∞–ª–≥–æ—Ä–∏—Ç–º–∞.

### üîß –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:

- ‚úÖ **–ù–∞—Å—Ç–æ—è—â–µ–µ RL –æ–±—É—á–µ–Ω–∏–µ** —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏
- ‚úÖ **GRPO –∞–ª–≥–æ—Ä–∏—Ç–º** —Å reward function –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
- ‚úÖ **LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã** –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning
- ‚úÖ **–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π** (0.0, 0.25, 0.5, 0.75, 1.0)
- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ** –º–æ–¥–µ–ª–µ–π –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
- ‚úÖ **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU** —Å flash attention
- ‚úÖ **–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ** –æ–±—É—á–µ–Ω–∏—è

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –ë–∞–∑–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è RL –æ–±—É—á–µ–Ω–∏—è
pip install trl peft bitsandbytes accelerate
```

### 2. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è

```bash
# –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å unsloth
python3 training/rl_trainer.py --model unsloth/Qwen2.5-3B-Instruct --max-steps 250

# –° –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
python3 training/rl_trainer.py \
  --model unsloth/Qwen2.5-3B-Instruct \
  --max-steps 500 \
  --learning-rate 5e-6 \
  --batch-size 1 \
  --num-generations 8 \
  --lora-r 64 \
  --output-dir ./dc_circuit_model_rl

# –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (7B+)
python3 training/rl_trainer.py \
  --model unsloth/Qwen2.5-7B-Instruct \
  --max-steps 1000 \
  --batch-size 1 \
  --lora-r 128 \
  --gpu-memory-utilization 0.8
```

### 3. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é |
|----------|----------|---------------|
| `--model` | –ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ | unsloth/Qwen2.5-3B-Instruct |
| `--output-dir` | –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ | ./dc_circuit_model_rl |
| `--max-seq-length` | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ | 1024 |
| `--max-steps` | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ | 250 |
| `--learning-rate` | –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è | 5e-6 |
| `--batch-size` | –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ | 1 |
| `--gradient-accumulation` | Gradient accumulation steps | 1 |
| `--num-generations` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –Ω–∞ —à–∞–≥ | 8 |
| `--lora-r` | LoRA rank | 64 |
| `--lora-alpha` | LoRA alpha | 64 |
| `--lora-dropout` | LoRA dropout | 0.05 |
| `--load-in-4bit` | 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è | True |
| `--fast-inference` | vLLM —É—Å–∫–æ—Ä–µ–Ω–∏–µ | True |
| `--gpu-memory-utilization` | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏ | 0.9 |
| `--temperature` | Sampling temperature | 0.7 |
| `--beta` | KL divergence coefficient | 0.04 |

## üîç –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

### **1. GRPO –ê–ª–≥–æ—Ä–∏—Ç–º**

**GRPO** = Generative Reward Policy Optimization

**–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:**
- –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
- –ö–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è reward function
- –ú–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è

### **2. Reward Function**

```python
def reward_function(completions, prompts, **kwargs):
    for completion, prompt in zip(completions, prompts):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ <gold> —Ç–µ–≥–æ–≤
        gold = extract_from_gold_tags(prompt)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ <answer> —Ç–µ–≥–æ–≤
        model_answer = extract_from_answer_tags(completion)

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        accuracy = verifier.get_accuracy_score(data_obj, model_answer)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ
        reward = accuracy  # 0.0, 0.25, 0.5, 0.75 –∏–ª–∏ 1.0
```

### **3. –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö**

**–ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:**
```text
You are an expert circuit analysis engineer...

Question: –ù–∞–π–¥–∏—Ç–µ —Ç–æ–∫ —á–µ—Ä–µ–∑ R1 (–≤ –ê–º–ø–µ—Ä–∞—Ö)
Answer: <gold>0.123</gold>
```

**–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏:**
```text
<think>–†–∞—Å—Å—É–∂–¥–∞–µ–º —à–∞–≥ –∑–∞ —à–∞–≥–æ–º...</think>
<answer>0.123</answer>
```

### **4. GRPO + LoRA + unsloth = –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**

**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –Ω–∞—à–µ–º –∫–æ–¥–µ:**

1. **–ó–∞–≥—Ä—É–∑–∫–∞ —Å unsloth (—É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è):**
   ```python
   model, tokenizer = FastLanguageModel.from_pretrained(
       "unsloth/Qwen2.5-3B-Instruct",
       max_seq_length=1024,
       load_in_4bit=True,      # 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
       fast_inference=True,    # vLLM —É—Å–∫–æ—Ä–µ–Ω–∏–µ
       max_lora_rank=64
   )
   ```

2. **–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã:**
   ```python
   model = FastLanguageModel.get_peft_model(
       model,
       r=64,                           # –í—ã—Å–æ–∫–∏–π rank –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
       target_modules=[
           "q_proj", "k_proj", "v_proj", "o_proj",
           "gate_proj", "up_proj", "down_proj"  # –í—Å–µ –º–æ–¥—É–ª–∏
       ],
       lora_alpha=64,
       use_gradient_checkpointing="unsloth"
   )
   ```

3. **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ Reward Functions:**
   ```python
   reward_functions = [
       xml_count_reward_func,      # –ü–æ–¥—Å—á–µ—Ç XML —Ç–µ–≥–æ–≤
       soft_format_reward_func,    # –ú—è–≥–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
       strict_format_reward_func,  # –°—Ç—Ä–æ–≥–∏–π —Ñ–æ—Ä–º–∞—Ç
       numeric_reward_func,        # –ß–∏—Å–ª–æ–≤–æ–π –æ—Ç–≤–µ—Ç
       correctness_reward_func     # –¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (2.0/0.0)
   ]
   ```

4. **GRPO —Å vLLM —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º:**
   ```python
   trainer = GRPOTrainer(
       model=model,
       processing_class=tokenizer,
       reward_funcs=reward_functions,  # 5 reward functions
       args=GRPOConfig(use_vllm=True), # vLLM –¥–ª—è inference
       train_dataset=dataset
   )
   ```

### **5. vLLM –∏ unsloth –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**

**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–æ–¥–µ:**

#### **unsloth (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):**
```python
# –ï—Å–ª–∏ unsloth —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-3B-Instruct",
    max_seq_length=1024,
    load_in_4bit=True,      # 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
    fast_inference=True,    # vLLM —É—Å–∫–æ—Ä–µ–Ω–∏–µ
    max_lora_rank=64,       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π rank –¥–ª—è LoRA
    gpu_memory_utilization=0.9
)
```

#### **vLLM –≤ GRPO:**
```python
# –í GRPOTrainer:
training_args = GRPOConfig(
    use_vllm=True,  # ‚Üê –í–ö–õ–Æ–ß–ê–ï–¢ vLLM –î–õ–Ø INFERENCE
    num_generations=8,  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 8 –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—Ä–æ–º–ø—Ç
    max_prompt_length=256,
    max_completion_length=200
)
```

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç vLLM:**
- ‚ö° **–£—Å–∫–æ—Ä—è–µ—Ç inference** –≤ 10-100 —Ä–∞–∑
- üéØ **–ü–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–æ–≤** –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
- üß† **–ù–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ** - —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å inference

#### **Fallback (–µ—Å–ª–∏ unsloth –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω):**
```python
# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
model = AutoModelForCausalLM.from_pretrained(...)
tokenizer = AutoTokenizer.from_pretrained(...)
# –†–∞–±–æ—Ç–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ
```

### **6. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ Reward Functions**

**–°–∏—Å—Ç–µ–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π:**
- **XML Count (0.5 –±–∞–ª–ª–∞)**: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤
- **Soft Format (0.5 –±–∞–ª–ª–∞)**: –ù–∞–ª–∏—á–∏–µ —Ç–µ–≥–æ–≤ –≤ –ª—é–±–æ–º –ø–æ—Ä—è–¥–∫–µ
- **Strict Format (0.5 –±–∞–ª–ª–∞)**: –¢–µ–≥–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
- **Numeric (0.5 –±–∞–ª–ª–∞)**: –û—Ç–≤–µ—Ç —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º
- **Correctness (2.0 –±–∞–ª–ª–∞)**: –ü–æ–ª–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞

**–ú–∞–∫—Å–∏–º—É–º:** 4.5 –±–∞–ª–ª–∞ –∑–∞ –∏–¥–µ–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

### **–õ–æ–≥–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:**
```
ü§ñ RL –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ê–ù–ê–õ–ò–ó–ê –¶–ï–ü–ï–ô
============================================================
üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RL —Ç—Ä–µ–Ω–µ—Ä–∞...
üìã –ú–æ–¥–µ–ª—å: Qwen/Qwen2.5-1.5B-Instruct
üéØ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: ./dc_circuit_model_rl
üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...
‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
üéØ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GRPO —Ç—Ä–µ–Ω–µ—Ä–∞...
‚úÖ –¢—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω
üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï!
============================================================
üìä –®–∞–≥ 10/500 | –°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ: 0.234
üìä –®–∞–≥ 20/500 | –°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ: 0.456
üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ —à–∞–≥–µ 200
‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!
üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: ./dc_circuit_model_rl
```

### **–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:**

1. **`training_stats.json`** - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
2. **`adapter_model.bin`** - LoRA –∞–¥–∞–ø—Ç–µ—Ä
3. **`adapter_config.json`** - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
4. **`tokenizer.json`** - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä

## üé® –û—Ü–µ–Ω–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

```python
from training.evaluate import generate_full_report

# –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
baseline_results, trained_results = generate_full_report(
    baseline_model="Qwen/Qwen2.5-1.5B-Instruct",
    trained_model="./dc_circuit_model_rl"
)
```

## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

### **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**
```txt
torch>=2.0.0
transformers>=4.36.0
trl>=0.8.0
accelerate>=0.25.0
peft>=0.7.0
bitsandbytes>=0.41.0
```

### **GPU —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- **–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ:** 8GB VRAM –¥–ª—è 1.5B –º–æ–¥–µ–ª–∏
- **–†–µ–∫–æ–º–µ–Ω–¥—É—é:** 16GB+ VRAM –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:** Flash Attention 2, bfloat16

### **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:**
- **100 —à–∞–≥–æ–≤:** ~5-10 –º–∏–Ω—É—Ç
- **500 —à–∞–≥–æ–≤:** ~30-60 –º–∏–Ω—É—Ç
- **1000 —à–∞–≥–æ–≤:** ~1-2 —á–∞—Å–∞

## üö® –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

### **–û—Ç–ª–∏—á–∏—è –æ—Ç —Å–∏–º—É–ª—è—Ü–∏–∏:**
- ‚úÖ **–ù–∞—Å—Ç–æ—è—â–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤** –º–æ–¥–µ–ª–∏
- ‚úÖ **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU** –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
- ‚úÖ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤** LoRA
- ‚úÖ **–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ**

### **‚ö†Ô∏è –¢—Ä–µ–±—É–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**

#### **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ (—Ä–∞–±–æ—Ç–∞—é—Ç –±–µ–∑ GPU):**
```bash
pip install torch transformers trl peft accelerate
```

#### **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ (—Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º):**
```bash
pip install unsloth trl peft bitsandbytes accelerate
```

#### **–ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä:**
```bash
pip install torch transformers unsloth trl peft accelerate bitsandbytes
```

### **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ GPU –ø–∞–º—è—Ç–∏ (8GB+ –¥–ª—è 1.5B –º–æ–¥–µ–ª–∏)
- ‚ö†Ô∏è –î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (30-60 –º–∏–Ω –¥–ª—è 500 —à–∞–≥–æ–≤)
- ‚ö†Ô∏è –°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç CUDA –¥–ª—è GPU —É—Å–∫–æ—Ä–µ–Ω–∏—è

## üéâ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–æ–∫–∞–∑–∞—Ç—å **–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ** –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π —Å **–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –æ—Ü–µ–Ω–∫–∏** –≤–º–µ—Å—Ç–æ –±–∏–Ω–∞—Ä–Ω–æ–π.

**–ü—Ä–∏–º–µ—Ä —É–ª—É—á—à–µ–Ω–∏—è:**
- **–î–æ:** 45% —Ç–æ—á–Ω–æ—Å—Ç—å
- **–ü–æ—Å–ª–µ:** 78% —Ç–æ—á–Ω–æ—Å—Ç—å
- **–£–ª—É—á—à–µ–Ω–∏–µ:** +33%

---

*–°–æ–∑–¥–∞–Ω–æ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π —Å reinforcement learning* üöÄ‚ö°
