#!/bin/bash
# Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ DDP Ğ½Ğ° 2x T4 GPU

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ DDP Ğ½Ğ° 2 GPU"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° GPU
NUM_GPUS=$(python -c "import torch; print(torch.cuda.device_count())")
echo "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ GPU: $NUM_GPUS"

if [ "$NUM_GPUS" -lt 2 ]; then
    echo "âŒ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ 2 GPU! Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ½Ğ° 1 GPU..."
    python training/rl_trainer.py
else
    echo "âœ… Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ DDP Ğ½Ğ° $NUM_GPUS GPU..."
    
    # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: torchrun (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)
    if command -v torchrun &> /dev/null; then
        echo "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ torchrun..."
        torchrun --nproc_per_node=$NUM_GPUS training/rl_trainer.py
    # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2: accelerate
    elif command -v accelerate &> /dev/null; then
        echo "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ accelerate..."
        accelerate launch --num_processes=$NUM_GPUS training/rl_trainer.py
    # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 3: python -m torch.distributed.launch
    else
        echo "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ torch.distributed.launch..."
        python -m torch.distributed.launch --nproc_per_node=$NUM_GPUS training/rl_trainer.py
    fi
fi

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "âœ… ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
